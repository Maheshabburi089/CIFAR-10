{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "82RTSaMu0H86"
      },
      "outputs": [],
      "source": [
        "%matplotlib inline\n",
        "import numpy as np\n",
        "import matplotlib\n",
        "from matplotlib import pyplot as plt\n",
        "from keras.models import Sequential\n",
        "from keras.optimizers import Adam, SGD\n",
        "from keras.callbacks import ModelCheckpoint\n",
        "from keras.constraints import maxnorm\n",
        "from keras.models import load_model\n",
        "from keras.layers import GlobalAveragePooling2D, Lambda, Conv2D, MaxPooling2D, Dropout, Dense, Flatten, Activation\n",
        "from keras.preprocessing.image import ImageDataGenerator\n",
        "from tensorflow.keras.utils import to_categorical"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "E3wAq8QQ0QnS"
      },
      "outputs": [],
      "source": [
        "matplotlib.style.use('ggplot')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "QYpBJXmF3QB2"
      },
      "outputs": [],
      "source": [
        "# To decode the files\n",
        "import pickle\n",
        "# For array manipulations\n",
        "import numpy as np\n",
        "# To make one-hot vectors\n",
        "from keras.utils import np_utils\n",
        "# To plot graphs and display images\n",
        "from matplotlib import pyplot as plt\n",
        "\n",
        "\n",
        "#constants\n",
        "\n",
        "path = \"data/\"  # Path to data \n",
        "\n",
        "# Height or width of the images (32 x 32)\n",
        "size = 32 \n",
        "\n",
        "# 3 channels: Red, Green, Blue (RGB)\n",
        "channels = 3  \n",
        "\n",
        "# Number of classes\n",
        "num_classes = 10 \n",
        "\n",
        "# Each file contains 10000 images\n",
        "image_batch = 10000 \n",
        "\n",
        "# 5 training files\n",
        "num_files_train = 5  \n",
        "\n",
        "# Total number of training images\n",
        "images_train = image_batch * num_files_train\n",
        "\n",
        "# https://www.cs.toronto.edu/~kriz/cifar.html\n",
        "\n",
        "\n",
        "def unpickle(file):  \n",
        "    \n",
        "    # Convert byte stream to object\n",
        "    with open(path + file,'rb') as fo:\n",
        "        print(\"Decoding file: %s\" % (path+file))\n",
        "        dict = pickle.load(fo, encoding='bytes')\n",
        "       \n",
        "    # Dictionary with images and labels\n",
        "    return dict\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "def convert_images(raw_images):\n",
        "    \n",
        "    # Convert images to numpy arrays\n",
        "    \n",
        "    # Convert raw images to numpy array and normalize it\n",
        "    raw = np.array(raw_images, dtype = float) / 255.0\n",
        "    \n",
        "    # Reshape to 4-dimensions - [image_number, channel, height, width]\n",
        "    images = raw.reshape([-1, channels, size, size])\n",
        "\n",
        "    images = images.transpose([0, 2, 3, 1])\n",
        "\n",
        "    # 4D array - [image_number, height, width, channel]\n",
        "    return images\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "def load_data(file):\n",
        "    # Load file, unpickle it and return images with their labels\n",
        "    \n",
        "    data = unpickle(file)\n",
        "    \n",
        "    # Get raw images\n",
        "    images_array = data[b'data']\n",
        "    \n",
        "    # Convert image\n",
        "    images = convert_images(images_array)\n",
        "    # Convert class number to numpy array\n",
        "    labels = np.array(data[b'labels'])\n",
        "        \n",
        "    # Images and labels in np array form\n",
        "    return images, labels\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "def get_test_data():\n",
        "    # Load all test data\n",
        "    \n",
        "    images, labels = load_data(file = \"test_batch\")\n",
        "    \n",
        "    # Images, their labels and \n",
        "    # corresponding one-hot vectors in form of np arrays\n",
        "    return images, labels, np_utils.to_categorical(labels,num_classes)\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "def get_train_data():\n",
        "    # Load all training data in 5 files\n",
        "    \n",
        "    # Pre-allocate arrays\n",
        "    images = np.zeros(shape = [images_train, size, size, channels], dtype = float)\n",
        "    labels = np.zeros(shape=[images_train],dtype = int)\n",
        "    \n",
        "    # Starting index of training dataset\n",
        "    start = 0\n",
        "    \n",
        "    # For all 5 files\n",
        "    for i in range(num_files_train):\n",
        "        \n",
        "        # Load images and labels\n",
        "        images_batch, labels_batch = load_data(file = \"data_batch_\" + str(i+1))\n",
        "        \n",
        "        # Calculate end index for current batch\n",
        "        end = start + image_batch\n",
        "        \n",
        "        # Store data to corresponding arrays\n",
        "        images[start:end,:] = images_batch        \n",
        "        labels[start:end] = labels_batch\n",
        "        \n",
        "        # Update starting index of next batch\n",
        "        start = end\n",
        "    \n",
        "    # Images, their labels and \n",
        "    # corresponding one-hot vectors in form of np arrays\n",
        "    return images, labels, np_utils.to_categorical(labels,num_classes)\n",
        "        \n",
        "\n",
        "\n",
        "def get_class_names():\n",
        "\n",
        "    # Load class names\n",
        "    raw = unpickle(\"batches.meta\")[b'label_names']\n",
        "\n",
        "    # Convert from binary strings\n",
        "    names = [x.decode('utf-8') for x in raw]\n",
        "\n",
        "    # Class names\n",
        "    return names\n",
        "\n",
        "\n",
        "\n",
        "def plot_images(images, labels_true, class_names, labels_pred=None):\n",
        "\n",
        "    assert len(images) == len(labels_true)\n",
        "\n",
        "    # Create a figure with sub-plots\n",
        "    fig, axes = plt.subplots(3, 3, figsize = (8,8))\n",
        "\n",
        "    # Adjust the vertical spacing\n",
        "    if labels_pred is None:\n",
        "        hspace = 0.2\n",
        "    else:\n",
        "        hspace = 0.5\n",
        "    fig.subplots_adjust(hspace=hspace, wspace=0.3)\n",
        "\n",
        "    for i, ax in enumerate(axes.flat):\n",
        "        # Fix crash when less than 9 images\n",
        "        if i < len(images):\n",
        "            # Plot the image\n",
        "            ax.imshow(images[i], interpolation='spline16')\n",
        "            \n",
        "            # Name of the true class\n",
        "            labels_true_name = class_names[labels_true[i]]\n",
        "\n",
        "            # Show true and predicted classes\n",
        "            if labels_pred is None:\n",
        "                xlabel = \"True: \"+labels_true_name\n",
        "            else:\n",
        "                # Name of the predicted class\n",
        "                labels_pred_name = class_names[labels_pred[i]]\n",
        "\n",
        "                xlabel = \"True: \"+labels_true_name+\"\\nPredicted: \"+ labels_pred_name\n",
        "\n",
        "            # Show the class on the x-axis\n",
        "            ax.set_xlabel(xlabel)\n",
        "        \n",
        "        # Remove ticks from the plot\n",
        "        ax.set_xticks([])\n",
        "        ax.set_yticks([])\n",
        "    \n",
        "    # Show the plot\n",
        "    plt.show()\n",
        "    \n",
        "\n",
        "def plot_model(model_details):\n",
        "\n",
        "    # Create sub-plots\n",
        "    fig, axs = plt.subplots(1,2,figsize=(15,5))\n",
        "    \n",
        "    # Summarize history for accuracy\n",
        "    axs[0].plot(range(1,len(model_details.history['acc'])+1),model_details.history['acc'])\n",
        "    axs[0].plot(range(1,len(model_details.history['val_acc'])+1),model_details.history['val_acc'])\n",
        "    axs[0].set_title('Model Accuracy')\n",
        "    axs[0].set_ylabel('Accuracy')\n",
        "    axs[0].set_xlabel('Epoch')\n",
        "    axs[0].set_xticks(np.arange(1,len(model_details.history['acc'])+1),len(model_details.history['acc'])/10)\n",
        "    axs[0].legend(['train', 'val'], loc='best')\n",
        "    \n",
        "    # Summarize history for loss\n",
        "    axs[1].plot(range(1,len(model_details.history['loss'])+1),model_details.history['loss'])\n",
        "    axs[1].plot(range(1,len(model_details.history['val_loss'])+1),model_details.history['val_loss'])\n",
        "    axs[1].set_title('Model Loss')\n",
        "    axs[1].set_ylabel('Loss')\n",
        "    axs[1].set_xlabel('Epoch')\n",
        "    axs[1].set_xticks(np.arange(1,len(model_details.history['loss'])+1),len(model_details.history['loss'])/10)\n",
        "    axs[1].legend(['train', 'val'], loc='best')\n",
        "    \n",
        "    # Show the plot\n",
        "    plt.show()\n",
        "\n",
        "\n",
        "\n",
        "def visualize_errors(images_test, labels_test, class_names, labels_pred, correct):\n",
        "    \n",
        "    incorrect = (correct == False)\n",
        "    \n",
        "    # Images of the test-set that have been incorrectly classified.\n",
        "    images_error = images_test[incorrect]\n",
        "    \n",
        "    # Get predicted classes for those images\n",
        "    labels_error = labels_pred[incorrect]\n",
        "\n",
        "    # Get true classes for those images\n",
        "    labels_true = labels_test[incorrect]\n",
        "    \n",
        "    \n",
        "    # Plot the first 9 images.\n",
        "    plot_images(images=images_error[0:9],\n",
        "                labels_true=labels_true[0:9],\n",
        "                class_names=class_names,\n",
        "                labels_pred=labels_error[0:9])\n",
        "    \n",
        "    \n",
        "def predict_classes(model, images_test, labels_test):\n",
        "    \n",
        "    # Predict class of image using model\n",
        "    class_pred = model.predict(images_test, batch_size=32)\n",
        "\n",
        "    # Convert vector to a label\n",
        "    labels_pred = np.argmax(class_pred,axis=1)\n",
        "\n",
        "    # Boolean array that tell if predicted label is the true label\n",
        "    correct = (labels_pred == labels_test)\n",
        "\n",
        "    # Array which tells if the prediction is correct or not\n",
        "    # And predicted labels\n",
        "    return correct, labels_pred"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6CNmT3910UP8"
      },
      "outputs": [],
      "source": [
        "# Hight and width of the images\n",
        "IMAGE_SIZE = 32\n",
        "# 3 channels, Red, Green and Blue\n",
        "CHANNELS = 3\n",
        "# Number of epochs\n",
        "NUM_EPOCH = 350\n",
        "# learning rate\n",
        "LEARN_RATE = 1.0e-4\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "XFngTjIf1P0A",
        "outputId": "39c50806-f117-40e5-a7b3-64a12baf0b77"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Downloading data from https://www.cs.toronto.edu/~kriz/cifar-10-python.tar.gz\n",
            "170498071/170498071 [==============================] - 2s 0us/step\n"
          ]
        }
      ],
      "source": [
        "# Importing the CIFAR-10 dataset from Keras \n",
        "from tensorflow.keras.datasets import cifar10\n",
        "(X_train, Y_train), (X_test, Y_test) = cifar10.load_data()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "lqZyoS4B1SBM"
      },
      "outputs": [],
      "source": [
        "# Normalizing\n",
        "X_train = X_train/255\n",
        "X_test = X_test/255\n",
        "\n",
        "# One-Hot-Encoding\n",
        "Y_train_en = to_categorical(Y_train,10)\n",
        "Y_test_en = to_categorical(Y_test,10)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "U85ScZI_0ibT"
      },
      "outputs": [],
      "source": [
        "\n",
        "def pure_cnn_model():\n",
        "    \n",
        "    model = Sequential()\n",
        "    \n",
        "    model.add(Conv2D(96, (3, 3), activation='relu', padding = 'same', input_shape=(IMAGE_SIZE,IMAGE_SIZE,CHANNELS)))    \n",
        "    model.add(Dropout(0.2))\n",
        "    \n",
        "    model.add(Conv2D(96, (3, 3), activation='relu', padding = 'same'))  \n",
        "    model.add(Conv2D(96, (3, 3), activation='relu', padding = 'same', strides = 2))    \n",
        "    model.add(Dropout(0.5))\n",
        "    \n",
        "    model.add(Conv2D(192, (3, 3), activation='relu', padding = 'same'))    \n",
        "    model.add(Conv2D(192, (3, 3), activation='relu', padding = 'same'))\n",
        "    model.add(Conv2D(192, (3, 3), activation='relu', padding = 'same', strides = 2))    \n",
        "    model.add(Dropout(0.5))    \n",
        "    \n",
        "    model.add(Conv2D(192, (3, 3), padding = 'same'))\n",
        "    model.add(Activation('relu'))\n",
        "    model.add(Conv2D(192, (1, 1),padding='valid'))\n",
        "    model.add(Activation('relu'))\n",
        "    model.add(Conv2D(10, (1, 1), padding='valid'))\n",
        "\n",
        "    model.add(GlobalAveragePooling2D())\n",
        "    \n",
        "    model.add(Activation('softmax'))\n",
        "\n",
        "    model.summary()\n",
        "    \n",
        "    return model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jEWChTLK0mZ6",
        "outputId": "7d9b7347-49ce-4907-b02f-60e51bc7ca6b"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Model: \"sequential\"\n",
            "_________________________________________________________________\n",
            " Layer (type)                Output Shape              Param #   \n",
            "=================================================================\n",
            " conv2d (Conv2D)             (None, 32, 32, 96)        2688      \n",
            "                                                                 \n",
            " dropout (Dropout)           (None, 32, 32, 96)        0         \n",
            "                                                                 \n",
            " conv2d_1 (Conv2D)           (None, 32, 32, 96)        83040     \n",
            "                                                                 \n",
            " conv2d_2 (Conv2D)           (None, 16, 16, 96)        83040     \n",
            "                                                                 \n",
            " dropout_1 (Dropout)         (None, 16, 16, 96)        0         \n",
            "                                                                 \n",
            " conv2d_3 (Conv2D)           (None, 16, 16, 192)       166080    \n",
            "                                                                 \n",
            " conv2d_4 (Conv2D)           (None, 16, 16, 192)       331968    \n",
            "                                                                 \n",
            " conv2d_5 (Conv2D)           (None, 8, 8, 192)         331968    \n",
            "                                                                 \n",
            " dropout_2 (Dropout)         (None, 8, 8, 192)         0         \n",
            "                                                                 \n",
            " conv2d_6 (Conv2D)           (None, 8, 8, 192)         331968    \n",
            "                                                                 \n",
            " activation (Activation)     (None, 8, 8, 192)         0         \n",
            "                                                                 \n",
            " conv2d_7 (Conv2D)           (None, 8, 8, 192)         37056     \n",
            "                                                                 \n",
            " activation_1 (Activation)   (None, 8, 8, 192)         0         \n",
            "                                                                 \n",
            " conv2d_8 (Conv2D)           (None, 8, 8, 10)          1930      \n",
            "                                                                 \n",
            " global_average_pooling2d (G  (None, 10)               0         \n",
            " lobalAveragePooling2D)                                          \n",
            "                                                                 \n",
            " activation_2 (Activation)   (None, 10)                0         \n",
            "                                                                 \n",
            "=================================================================\n",
            "Total params: 1,369,738\n",
            "Trainable params: 1,369,738\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n"
          ]
        }
      ],
      "source": [
        "model = pure_cnn_model()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "TgQbMikN0oVQ"
      },
      "outputs": [],
      "source": [
        "checkpoint = ModelCheckpoint('best_model_improved.h5',  # model filename\n",
        "                             monitor='val_loss', # quantity to monitor\n",
        "                             verbose=0, # verbosity - 0 or 1\n",
        "                             save_best_only= True, # The latest best model will not be overwritten\n",
        "                             mode='auto') # The decision to overwrite model is made \n",
        "                                          # automatically depending on the quantity to monitor "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "d6TU2gOB0qbu",
        "outputId": "0bb07371-e2bc-45c4-a288-2e17bc33f22f"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.8/dist-packages/keras/optimizers/optimizer_v2/adam.py:110: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.\n",
            "  super(Adam, self).__init__(name, **kwargs)\n"
          ]
        }
      ],
      "source": [
        "model.compile(loss='categorical_crossentropy', # Better loss function for neural networks\n",
        "              optimizer=Adam(lr=LEARN_RATE), # Adam optimizer with 1.0e-4 learning rate\n",
        "              metrics = ['accuracy']) # Metrics to be evaluated by the model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4RYPCvPI0sva",
        "outputId": "bb5d15ea-454f-4dab-c542-85744142e346"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 1/350\n",
            "391/391 [==============================] - 26s 64ms/step - loss: 1.9887 - accuracy: 0.2388 - val_loss: 1.8209 - val_accuracy: 0.3063\n",
            "Epoch 2/350\n",
            "391/391 [==============================] - 24s 62ms/step - loss: 1.6760 - accuracy: 0.3706 - val_loss: 1.7217 - val_accuracy: 0.3824\n",
            "Epoch 3/350\n",
            "391/391 [==============================] - 25s 64ms/step - loss: 1.5549 - accuracy: 0.4270 - val_loss: 1.4811 - val_accuracy: 0.4587\n",
            "Epoch 4/350\n",
            "391/391 [==============================] - 25s 64ms/step - loss: 1.4807 - accuracy: 0.4554 - val_loss: 1.4377 - val_accuracy: 0.4796\n",
            "Epoch 5/350\n",
            "391/391 [==============================] - 24s 61ms/step - loss: 1.4201 - accuracy: 0.4813 - val_loss: 1.3450 - val_accuracy: 0.5161\n",
            "Epoch 6/350\n",
            "391/391 [==============================] - 24s 61ms/step - loss: 1.3538 - accuracy: 0.5074 - val_loss: 1.3212 - val_accuracy: 0.5235\n",
            "Epoch 7/350\n",
            "391/391 [==============================] - 23s 60ms/step - loss: 1.3103 - accuracy: 0.5275 - val_loss: 1.2247 - val_accuracy: 0.5606\n",
            "Epoch 8/350\n",
            "391/391 [==============================] - 24s 62ms/step - loss: 1.2671 - accuracy: 0.5437 - val_loss: 1.2397 - val_accuracy: 0.5620\n",
            "Epoch 9/350\n",
            "391/391 [==============================] - 24s 61ms/step - loss: 1.2214 - accuracy: 0.5606 - val_loss: 1.1896 - val_accuracy: 0.5769\n",
            "Epoch 10/350\n",
            "391/391 [==============================] - 24s 61ms/step - loss: 1.1834 - accuracy: 0.5776 - val_loss: 1.1463 - val_accuracy: 0.5904\n",
            "Epoch 11/350\n",
            "391/391 [==============================] - 24s 61ms/step - loss: 1.1550 - accuracy: 0.5862 - val_loss: 1.1089 - val_accuracy: 0.6107\n",
            "Epoch 12/350\n",
            "391/391 [==============================] - 23s 60ms/step - loss: 1.1298 - accuracy: 0.5956 - val_loss: 1.1031 - val_accuracy: 0.6058\n",
            "Epoch 13/350\n",
            "391/391 [==============================] - 24s 61ms/step - loss: 1.0931 - accuracy: 0.6132 - val_loss: 1.0610 - val_accuracy: 0.6220\n",
            "Epoch 14/350\n",
            "391/391 [==============================] - 23s 60ms/step - loss: 1.0743 - accuracy: 0.6176 - val_loss: 1.0635 - val_accuracy: 0.6262\n",
            "Epoch 15/350\n",
            "391/391 [==============================] - 24s 60ms/step - loss: 1.0476 - accuracy: 0.6263 - val_loss: 1.0099 - val_accuracy: 0.6362\n",
            "Epoch 16/350\n",
            "391/391 [==============================] - 23s 59ms/step - loss: 1.0313 - accuracy: 0.6340 - val_loss: 1.0271 - val_accuracy: 0.6393\n",
            "Epoch 17/350\n",
            "391/391 [==============================] - 23s 60ms/step - loss: 1.0059 - accuracy: 0.6445 - val_loss: 0.9989 - val_accuracy: 0.6435\n",
            "Epoch 18/350\n",
            "391/391 [==============================] - 23s 60ms/step - loss: 0.9909 - accuracy: 0.6475 - val_loss: 0.9962 - val_accuracy: 0.6499\n",
            "Epoch 19/350\n",
            "391/391 [==============================] - 25s 63ms/step - loss: 0.9673 - accuracy: 0.6601 - val_loss: 0.9455 - val_accuracy: 0.6658\n",
            "Epoch 20/350\n",
            "391/391 [==============================] - 23s 59ms/step - loss: 0.9527 - accuracy: 0.6633 - val_loss: 0.9513 - val_accuracy: 0.6649\n",
            "Epoch 21/350\n",
            "391/391 [==============================] - 25s 63ms/step - loss: 0.9309 - accuracy: 0.6725 - val_loss: 0.9823 - val_accuracy: 0.6539\n",
            "Epoch 22/350\n",
            "391/391 [==============================] - 25s 63ms/step - loss: 0.9273 - accuracy: 0.6743 - val_loss: 0.9297 - val_accuracy: 0.6679\n",
            "Epoch 23/350\n",
            "391/391 [==============================] - 23s 60ms/step - loss: 0.9056 - accuracy: 0.6815 - val_loss: 0.9048 - val_accuracy: 0.6839\n",
            "Epoch 24/350\n",
            "391/391 [==============================] - 23s 60ms/step - loss: 0.8938 - accuracy: 0.6856 - val_loss: 0.8860 - val_accuracy: 0.6880\n",
            "Epoch 25/350\n",
            "391/391 [==============================] - 23s 60ms/step - loss: 0.8791 - accuracy: 0.6902 - val_loss: 0.8827 - val_accuracy: 0.6871\n",
            "Epoch 26/350\n",
            "391/391 [==============================] - 23s 59ms/step - loss: 0.8677 - accuracy: 0.6944 - val_loss: 0.8455 - val_accuracy: 0.6982\n",
            "Epoch 27/350\n",
            "391/391 [==============================] - 23s 59ms/step - loss: 0.8497 - accuracy: 0.7007 - val_loss: 0.8887 - val_accuracy: 0.6923\n",
            "Epoch 28/350\n",
            "391/391 [==============================] - 23s 60ms/step - loss: 0.8404 - accuracy: 0.7048 - val_loss: 0.8489 - val_accuracy: 0.7018\n",
            "Epoch 29/350\n",
            "391/391 [==============================] - 24s 60ms/step - loss: 0.8280 - accuracy: 0.7094 - val_loss: 0.8170 - val_accuracy: 0.7104\n",
            "Epoch 30/350\n",
            "391/391 [==============================] - 23s 60ms/step - loss: 0.8189 - accuracy: 0.7136 - val_loss: 0.8110 - val_accuracy: 0.7138\n",
            "Epoch 31/350\n",
            "391/391 [==============================] - 25s 63ms/step - loss: 0.8063 - accuracy: 0.7189 - val_loss: 0.8202 - val_accuracy: 0.7124\n",
            "Epoch 32/350\n",
            "391/391 [==============================] - 23s 60ms/step - loss: 0.7934 - accuracy: 0.7225 - val_loss: 0.8011 - val_accuracy: 0.7206\n",
            "Epoch 33/350\n",
            "391/391 [==============================] - 23s 60ms/step - loss: 0.7835 - accuracy: 0.7234 - val_loss: 0.7864 - val_accuracy: 0.7212\n",
            "Epoch 34/350\n",
            "391/391 [==============================] - 23s 59ms/step - loss: 0.7752 - accuracy: 0.7264 - val_loss: 0.7985 - val_accuracy: 0.7185\n",
            "Epoch 35/350\n",
            "391/391 [==============================] - 23s 59ms/step - loss: 0.7635 - accuracy: 0.7317 - val_loss: 0.7892 - val_accuracy: 0.7236\n",
            "Epoch 36/350\n",
            "391/391 [==============================] - 24s 60ms/step - loss: 0.7528 - accuracy: 0.7365 - val_loss: 0.7618 - val_accuracy: 0.7352\n",
            "Epoch 37/350\n",
            "391/391 [==============================] - 23s 60ms/step - loss: 0.7470 - accuracy: 0.7374 - val_loss: 0.8058 - val_accuracy: 0.7195\n",
            "Epoch 38/350\n",
            "391/391 [==============================] - 24s 60ms/step - loss: 0.7359 - accuracy: 0.7419 - val_loss: 0.7471 - val_accuracy: 0.7350\n",
            "Epoch 39/350\n",
            "391/391 [==============================] - 23s 60ms/step - loss: 0.7248 - accuracy: 0.7469 - val_loss: 0.7423 - val_accuracy: 0.7397\n",
            "Epoch 40/350\n",
            "391/391 [==============================] - 23s 60ms/step - loss: 0.7153 - accuracy: 0.7519 - val_loss: 0.7331 - val_accuracy: 0.7464\n",
            "Epoch 41/350\n",
            "391/391 [==============================] - 24s 60ms/step - loss: 0.7088 - accuracy: 0.7503 - val_loss: 0.7297 - val_accuracy: 0.7480\n",
            "Epoch 42/350\n",
            "391/391 [==============================] - 23s 60ms/step - loss: 0.6952 - accuracy: 0.7571 - val_loss: 0.7285 - val_accuracy: 0.7465\n",
            "Epoch 43/350\n",
            "391/391 [==============================] - 23s 60ms/step - loss: 0.6873 - accuracy: 0.7599 - val_loss: 0.7158 - val_accuracy: 0.7516\n",
            "Epoch 44/350\n",
            "391/391 [==============================] - 23s 60ms/step - loss: 0.6815 - accuracy: 0.7616 - val_loss: 0.7117 - val_accuracy: 0.7552\n",
            "Epoch 45/350\n",
            "391/391 [==============================] - 23s 59ms/step - loss: 0.6705 - accuracy: 0.7649 - val_loss: 0.7188 - val_accuracy: 0.7498\n",
            "Epoch 46/350\n",
            "391/391 [==============================] - 24s 61ms/step - loss: 0.6700 - accuracy: 0.7648 - val_loss: 0.7045 - val_accuracy: 0.7559\n",
            "Epoch 47/350\n",
            "391/391 [==============================] - 23s 60ms/step - loss: 0.6606 - accuracy: 0.7698 - val_loss: 0.7089 - val_accuracy: 0.7590\n",
            "Epoch 48/350\n",
            "391/391 [==============================] - 23s 59ms/step - loss: 0.6455 - accuracy: 0.7740 - val_loss: 0.7108 - val_accuracy: 0.7533\n",
            "Epoch 49/350\n",
            "391/391 [==============================] - 23s 59ms/step - loss: 0.6444 - accuracy: 0.7751 - val_loss: 0.6809 - val_accuracy: 0.7625\n",
            "Epoch 50/350\n",
            "391/391 [==============================] - 23s 60ms/step - loss: 0.6325 - accuracy: 0.7781 - val_loss: 0.6791 - val_accuracy: 0.7631\n",
            "Epoch 51/350\n",
            "391/391 [==============================] - 23s 59ms/step - loss: 0.6274 - accuracy: 0.7793 - val_loss: 0.7089 - val_accuracy: 0.7557\n",
            "Epoch 52/350\n",
            "391/391 [==============================] - 23s 60ms/step - loss: 0.6200 - accuracy: 0.7835 - val_loss: 0.6639 - val_accuracy: 0.7705\n",
            "Epoch 53/350\n",
            "391/391 [==============================] - 23s 60ms/step - loss: 0.6155 - accuracy: 0.7835 - val_loss: 0.6720 - val_accuracy: 0.7681\n",
            "Epoch 54/350\n",
            "391/391 [==============================] - 24s 60ms/step - loss: 0.6080 - accuracy: 0.7861 - val_loss: 0.6553 - val_accuracy: 0.7742\n",
            "Epoch 55/350\n",
            "391/391 [==============================] - 23s 60ms/step - loss: 0.5981 - accuracy: 0.7909 - val_loss: 0.6652 - val_accuracy: 0.7701\n",
            "Epoch 56/350\n",
            "391/391 [==============================] - 23s 60ms/step - loss: 0.5927 - accuracy: 0.7918 - val_loss: 0.6365 - val_accuracy: 0.7775\n",
            "Epoch 57/350\n",
            "391/391 [==============================] - 23s 60ms/step - loss: 0.5893 - accuracy: 0.7928 - val_loss: 0.6603 - val_accuracy: 0.7743\n",
            "Epoch 58/350\n",
            "391/391 [==============================] - 23s 60ms/step - loss: 0.5824 - accuracy: 0.7948 - val_loss: 0.6525 - val_accuracy: 0.7723\n",
            "Epoch 59/350\n",
            "391/391 [==============================] - 23s 59ms/step - loss: 0.5702 - accuracy: 0.7992 - val_loss: 0.6584 - val_accuracy: 0.7735\n",
            "Epoch 60/350\n",
            "391/391 [==============================] - 23s 60ms/step - loss: 0.5731 - accuracy: 0.7993 - val_loss: 0.6319 - val_accuracy: 0.7838\n",
            "Epoch 61/350\n",
            "391/391 [==============================] - 23s 60ms/step - loss: 0.5618 - accuracy: 0.8029 - val_loss: 0.6371 - val_accuracy: 0.7807\n",
            "Epoch 62/350\n",
            "391/391 [==============================] - 23s 60ms/step - loss: 0.5564 - accuracy: 0.8039 - val_loss: 0.6906 - val_accuracy: 0.7639\n",
            "Epoch 63/350\n",
            "391/391 [==============================] - 23s 60ms/step - loss: 0.5453 - accuracy: 0.8081 - val_loss: 0.6345 - val_accuracy: 0.7812\n",
            "Epoch 64/350\n",
            "391/391 [==============================] - 23s 60ms/step - loss: 0.5457 - accuracy: 0.8070 - val_loss: 0.6201 - val_accuracy: 0.7893\n",
            "Epoch 65/350\n",
            "391/391 [==============================] - 23s 60ms/step - loss: 0.5393 - accuracy: 0.8101 - val_loss: 0.6457 - val_accuracy: 0.7825\n",
            "Epoch 66/350\n",
            "391/391 [==============================] - 23s 60ms/step - loss: 0.5290 - accuracy: 0.8141 - val_loss: 0.6171 - val_accuracy: 0.7830\n",
            "Epoch 67/350\n",
            "391/391 [==============================] - 23s 60ms/step - loss: 0.5277 - accuracy: 0.8149 - val_loss: 0.6149 - val_accuracy: 0.7904\n",
            "Epoch 68/350\n",
            "391/391 [==============================] - 23s 59ms/step - loss: 0.5200 - accuracy: 0.8169 - val_loss: 0.5942 - val_accuracy: 0.7950\n",
            "Epoch 69/350\n",
            "391/391 [==============================] - 23s 60ms/step - loss: 0.5145 - accuracy: 0.8184 - val_loss: 0.6325 - val_accuracy: 0.7850\n",
            "Epoch 70/350\n",
            "391/391 [==============================] - 23s 59ms/step - loss: 0.5079 - accuracy: 0.8201 - val_loss: 0.6128 - val_accuracy: 0.7901\n",
            "Epoch 71/350\n",
            "391/391 [==============================] - 23s 60ms/step - loss: 0.5039 - accuracy: 0.8229 - val_loss: 0.6042 - val_accuracy: 0.7920\n",
            "Epoch 72/350\n",
            "391/391 [==============================] - 23s 60ms/step - loss: 0.5018 - accuracy: 0.8222 - val_loss: 0.5887 - val_accuracy: 0.7956\n",
            "Epoch 73/350\n",
            "391/391 [==============================] - 23s 60ms/step - loss: 0.4921 - accuracy: 0.8263 - val_loss: 0.6062 - val_accuracy: 0.7953\n",
            "Epoch 74/350\n",
            "391/391 [==============================] - 23s 60ms/step - loss: 0.4856 - accuracy: 0.8293 - val_loss: 0.5995 - val_accuracy: 0.7950\n",
            "Epoch 75/350\n",
            "391/391 [==============================] - 23s 60ms/step - loss: 0.4815 - accuracy: 0.8304 - val_loss: 0.6201 - val_accuracy: 0.7910\n",
            "Epoch 76/350\n",
            "391/391 [==============================] - 23s 59ms/step - loss: 0.4804 - accuracy: 0.8302 - val_loss: 0.5948 - val_accuracy: 0.7975\n",
            "Epoch 77/350\n",
            "391/391 [==============================] - 23s 60ms/step - loss: 0.4772 - accuracy: 0.8304 - val_loss: 0.5778 - val_accuracy: 0.8050\n",
            "Epoch 78/350\n",
            "391/391 [==============================] - 23s 60ms/step - loss: 0.4685 - accuracy: 0.8355 - val_loss: 0.6072 - val_accuracy: 0.7967\n",
            "Epoch 79/350\n",
            "391/391 [==============================] - 23s 60ms/step - loss: 0.4628 - accuracy: 0.8382 - val_loss: 0.5845 - val_accuracy: 0.8029\n",
            "Epoch 80/350\n",
            "391/391 [==============================] - 23s 60ms/step - loss: 0.4586 - accuracy: 0.8384 - val_loss: 0.5711 - val_accuracy: 0.8043\n",
            "Epoch 81/350\n",
            "391/391 [==============================] - 23s 60ms/step - loss: 0.4566 - accuracy: 0.8389 - val_loss: 0.5764 - val_accuracy: 0.8014\n",
            "Epoch 82/350\n",
            "391/391 [==============================] - 23s 60ms/step - loss: 0.4452 - accuracy: 0.8438 - val_loss: 0.5702 - val_accuracy: 0.8075\n",
            "Epoch 83/350\n",
            "391/391 [==============================] - 23s 60ms/step - loss: 0.4414 - accuracy: 0.8444 - val_loss: 0.5922 - val_accuracy: 0.8018\n",
            "Epoch 84/350\n",
            "391/391 [==============================] - 23s 60ms/step - loss: 0.4374 - accuracy: 0.8462 - val_loss: 0.5787 - val_accuracy: 0.8030\n",
            "Epoch 85/350\n",
            "391/391 [==============================] - 23s 59ms/step - loss: 0.4378 - accuracy: 0.8463 - val_loss: 0.5794 - val_accuracy: 0.8061\n",
            "Epoch 86/350\n",
            "391/391 [==============================] - 23s 60ms/step - loss: 0.4321 - accuracy: 0.8481 - val_loss: 0.5620 - val_accuracy: 0.8087\n",
            "Epoch 87/350\n",
            "391/391 [==============================] - 23s 60ms/step - loss: 0.4276 - accuracy: 0.8477 - val_loss: 0.5589 - val_accuracy: 0.8129\n",
            "Epoch 88/350\n",
            "391/391 [==============================] - 23s 60ms/step - loss: 0.4232 - accuracy: 0.8520 - val_loss: 0.5704 - val_accuracy: 0.8069\n",
            "Epoch 89/350\n",
            "391/391 [==============================] - 25s 63ms/step - loss: 0.4146 - accuracy: 0.8538 - val_loss: 0.5775 - val_accuracy: 0.8081\n",
            "Epoch 90/350\n",
            "391/391 [==============================] - 23s 59ms/step - loss: 0.4131 - accuracy: 0.8551 - val_loss: 0.5634 - val_accuracy: 0.8126\n",
            "Epoch 91/350\n",
            "391/391 [==============================] - 25s 63ms/step - loss: 0.4071 - accuracy: 0.8567 - val_loss: 0.5554 - val_accuracy: 0.8137\n",
            "Epoch 92/350\n",
            "391/391 [==============================] - 23s 60ms/step - loss: 0.4045 - accuracy: 0.8576 - val_loss: 0.5835 - val_accuracy: 0.8069\n",
            "Epoch 93/350\n",
            "391/391 [==============================] - 23s 59ms/step - loss: 0.4001 - accuracy: 0.8580 - val_loss: 0.5872 - val_accuracy: 0.8084\n",
            "Epoch 94/350\n",
            "391/391 [==============================] - 23s 60ms/step - loss: 0.3947 - accuracy: 0.8600 - val_loss: 0.5836 - val_accuracy: 0.8066\n",
            "Epoch 95/350\n",
            "391/391 [==============================] - 23s 59ms/step - loss: 0.3940 - accuracy: 0.8610 - val_loss: 0.5669 - val_accuracy: 0.8171\n",
            "Epoch 96/350\n",
            "391/391 [==============================] - 24s 60ms/step - loss: 0.3905 - accuracy: 0.8632 - val_loss: 0.5446 - val_accuracy: 0.8178\n",
            "Epoch 97/350\n",
            "391/391 [==============================] - 23s 60ms/step - loss: 0.3843 - accuracy: 0.8635 - val_loss: 0.5446 - val_accuracy: 0.8195\n",
            "Epoch 98/350\n",
            "391/391 [==============================] - 23s 60ms/step - loss: 0.3782 - accuracy: 0.8654 - val_loss: 0.5648 - val_accuracy: 0.8112\n",
            "Epoch 99/350\n",
            "391/391 [==============================] - 23s 59ms/step - loss: 0.3789 - accuracy: 0.8640 - val_loss: 0.5452 - val_accuracy: 0.8196\n",
            "Epoch 100/350\n",
            "391/391 [==============================] - 23s 60ms/step - loss: 0.3717 - accuracy: 0.8673 - val_loss: 0.5930 - val_accuracy: 0.8075\n",
            "Epoch 101/350\n",
            "391/391 [==============================] - 23s 59ms/step - loss: 0.3652 - accuracy: 0.8717 - val_loss: 0.5658 - val_accuracy: 0.8189\n",
            "Epoch 102/350\n",
            "391/391 [==============================] - 23s 60ms/step - loss: 0.3648 - accuracy: 0.8700 - val_loss: 0.5416 - val_accuracy: 0.8194\n",
            "Epoch 103/350\n",
            "391/391 [==============================] - 23s 60ms/step - loss: 0.3661 - accuracy: 0.8705 - val_loss: 0.5542 - val_accuracy: 0.8150\n",
            "Epoch 104/350\n",
            "391/391 [==============================] - 25s 63ms/step - loss: 0.3552 - accuracy: 0.8743 - val_loss: 0.5387 - val_accuracy: 0.8234\n",
            "Epoch 105/350\n",
            "391/391 [==============================] - 23s 60ms/step - loss: 0.3570 - accuracy: 0.8734 - val_loss: 0.5657 - val_accuracy: 0.8172\n",
            "Epoch 106/350\n",
            "391/391 [==============================] - 23s 60ms/step - loss: 0.3485 - accuracy: 0.8738 - val_loss: 0.5799 - val_accuracy: 0.8105\n",
            "Epoch 107/350\n",
            "391/391 [==============================] - 23s 59ms/step - loss: 0.3435 - accuracy: 0.8786 - val_loss: 0.5548 - val_accuracy: 0.8171\n",
            "Epoch 108/350\n",
            "391/391 [==============================] - 23s 60ms/step - loss: 0.3407 - accuracy: 0.8803 - val_loss: 0.5625 - val_accuracy: 0.8139\n",
            "Epoch 109/350\n",
            "391/391 [==============================] - 23s 60ms/step - loss: 0.3403 - accuracy: 0.8800 - val_loss: 0.5578 - val_accuracy: 0.8226\n",
            "Epoch 110/350\n",
            "391/391 [==============================] - 23s 59ms/step - loss: 0.3314 - accuracy: 0.8836 - val_loss: 0.5891 - val_accuracy: 0.8139\n",
            "Epoch 111/350\n",
            "391/391 [==============================] - 23s 60ms/step - loss: 0.3366 - accuracy: 0.8796 - val_loss: 0.5401 - val_accuracy: 0.8187\n",
            "Epoch 112/350\n",
            "391/391 [==============================] - 23s 59ms/step - loss: 0.3264 - accuracy: 0.8833 - val_loss: 0.5460 - val_accuracy: 0.8245\n",
            "Epoch 113/350\n",
            "391/391 [==============================] - 25s 63ms/step - loss: 0.3277 - accuracy: 0.8832 - val_loss: 0.5751 - val_accuracy: 0.8157\n",
            "Epoch 114/350\n",
            "391/391 [==============================] - 23s 60ms/step - loss: 0.3211 - accuracy: 0.8851 - val_loss: 0.5451 - val_accuracy: 0.8235\n",
            "Epoch 115/350\n",
            "391/391 [==============================] - 23s 60ms/step - loss: 0.3143 - accuracy: 0.8885 - val_loss: 0.5461 - val_accuracy: 0.8253\n",
            "Epoch 116/350\n",
            "391/391 [==============================] - 23s 60ms/step - loss: 0.3191 - accuracy: 0.8862 - val_loss: 0.5550 - val_accuracy: 0.8203\n",
            "Epoch 117/350\n",
            "391/391 [==============================] - 23s 60ms/step - loss: 0.3091 - accuracy: 0.8895 - val_loss: 0.5555 - val_accuracy: 0.8228\n",
            "Epoch 118/350\n",
            "391/391 [==============================] - 23s 60ms/step - loss: 0.3083 - accuracy: 0.8903 - val_loss: 0.5539 - val_accuracy: 0.8214\n",
            "Epoch 119/350\n",
            "391/391 [==============================] - 23s 60ms/step - loss: 0.3098 - accuracy: 0.8894 - val_loss: 0.5659 - val_accuracy: 0.8202\n",
            "Epoch 120/350\n",
            "391/391 [==============================] - 23s 60ms/step - loss: 0.3070 - accuracy: 0.8909 - val_loss: 0.5534 - val_accuracy: 0.8242\n",
            "Epoch 121/350\n",
            "391/391 [==============================] - 23s 60ms/step - loss: 0.3049 - accuracy: 0.8918 - val_loss: 0.5533 - val_accuracy: 0.8257\n",
            "Epoch 122/350\n",
            "391/391 [==============================] - 23s 60ms/step - loss: 0.3013 - accuracy: 0.8928 - val_loss: 0.5437 - val_accuracy: 0.8261\n",
            "Epoch 123/350\n",
            "391/391 [==============================] - 23s 60ms/step - loss: 0.2977 - accuracy: 0.8933 - val_loss: 0.5703 - val_accuracy: 0.8193\n",
            "Epoch 124/350\n",
            "391/391 [==============================] - 23s 60ms/step - loss: 0.2909 - accuracy: 0.8952 - val_loss: 0.5620 - val_accuracy: 0.8234\n",
            "Epoch 125/350\n",
            "391/391 [==============================] - 23s 60ms/step - loss: 0.2907 - accuracy: 0.8962 - val_loss: 0.5808 - val_accuracy: 0.8215\n",
            "Epoch 126/350\n",
            "391/391 [==============================] - 23s 60ms/step - loss: 0.2914 - accuracy: 0.8955 - val_loss: 0.5727 - val_accuracy: 0.8175\n",
            "Epoch 127/350\n",
            "391/391 [==============================] - 23s 59ms/step - loss: 0.2839 - accuracy: 0.8984 - val_loss: 0.5523 - val_accuracy: 0.8272\n",
            "Epoch 128/350\n",
            "391/391 [==============================] - 23s 60ms/step - loss: 0.2831 - accuracy: 0.8988 - val_loss: 0.5463 - val_accuracy: 0.8267\n",
            "Epoch 129/350\n",
            "391/391 [==============================] - 23s 60ms/step - loss: 0.2761 - accuracy: 0.9010 - val_loss: 0.5484 - val_accuracy: 0.8266\n",
            "Epoch 130/350\n",
            "391/391 [==============================] - 24s 60ms/step - loss: 0.2779 - accuracy: 0.8996 - val_loss: 0.5392 - val_accuracy: 0.8305\n",
            "Epoch 131/350\n",
            "391/391 [==============================] - 25s 63ms/step - loss: 0.2726 - accuracy: 0.9013 - val_loss: 0.5677 - val_accuracy: 0.8250\n",
            "Epoch 132/350\n",
            "391/391 [==============================] - 23s 60ms/step - loss: 0.2704 - accuracy: 0.9040 - val_loss: 0.5581 - val_accuracy: 0.8285\n",
            "Epoch 133/350\n",
            "391/391 [==============================] - 23s 60ms/step - loss: 0.2687 - accuracy: 0.9027 - val_loss: 0.5480 - val_accuracy: 0.8316\n",
            "Epoch 134/350\n",
            "391/391 [==============================] - 23s 60ms/step - loss: 0.2641 - accuracy: 0.9064 - val_loss: 0.5496 - val_accuracy: 0.8295\n",
            "Epoch 135/350\n",
            "391/391 [==============================] - 23s 60ms/step - loss: 0.2615 - accuracy: 0.9065 - val_loss: 0.5599 - val_accuracy: 0.8286\n",
            "Epoch 136/350\n",
            "391/391 [==============================] - 25s 63ms/step - loss: 0.2660 - accuracy: 0.9053 - val_loss: 0.5592 - val_accuracy: 0.8208\n",
            "Epoch 137/350\n",
            "391/391 [==============================] - 23s 60ms/step - loss: 0.2535 - accuracy: 0.9076 - val_loss: 0.5584 - val_accuracy: 0.8285\n",
            "Epoch 138/350\n",
            "391/391 [==============================] - 24s 60ms/step - loss: 0.2534 - accuracy: 0.9095 - val_loss: 0.5801 - val_accuracy: 0.8225\n",
            "Epoch 139/350\n",
            "391/391 [==============================] - 23s 60ms/step - loss: 0.2505 - accuracy: 0.9100 - val_loss: 0.5679 - val_accuracy: 0.8281\n",
            "Epoch 140/350\n",
            "391/391 [==============================] - 23s 60ms/step - loss: 0.2462 - accuracy: 0.9113 - val_loss: 0.5897 - val_accuracy: 0.8281\n",
            "Epoch 141/350\n",
            "391/391 [==============================] - 23s 60ms/step - loss: 0.2445 - accuracy: 0.9110 - val_loss: 0.5807 - val_accuracy: 0.8228\n",
            "Epoch 142/350\n",
            "391/391 [==============================] - 23s 60ms/step - loss: 0.2519 - accuracy: 0.9091 - val_loss: 0.5954 - val_accuracy: 0.8185\n",
            "Epoch 143/350\n",
            "391/391 [==============================] - 23s 60ms/step - loss: 0.2455 - accuracy: 0.9108 - val_loss: 0.6178 - val_accuracy: 0.8219\n",
            "Epoch 144/350\n",
            "391/391 [==============================] - 23s 59ms/step - loss: 0.2457 - accuracy: 0.9130 - val_loss: 0.5783 - val_accuracy: 0.8273\n",
            "Epoch 145/350\n",
            "391/391 [==============================] - 23s 60ms/step - loss: 0.2366 - accuracy: 0.9141 - val_loss: 0.5622 - val_accuracy: 0.8320\n",
            "Epoch 146/350\n",
            "391/391 [==============================] - 23s 60ms/step - loss: 0.2338 - accuracy: 0.9164 - val_loss: 0.6104 - val_accuracy: 0.8247\n",
            "Epoch 147/350\n",
            "391/391 [==============================] - 23s 60ms/step - loss: 0.2321 - accuracy: 0.9163 - val_loss: 0.5737 - val_accuracy: 0.8277\n",
            "Epoch 148/350\n",
            "391/391 [==============================] - 23s 60ms/step - loss: 0.2307 - accuracy: 0.9158 - val_loss: 0.5791 - val_accuracy: 0.8308\n",
            "Epoch 149/350\n",
            "391/391 [==============================] - 23s 60ms/step - loss: 0.2315 - accuracy: 0.9168 - val_loss: 0.5630 - val_accuracy: 0.8314\n",
            "Epoch 150/350\n",
            "391/391 [==============================] - 23s 60ms/step - loss: 0.2253 - accuracy: 0.9180 - val_loss: 0.5686 - val_accuracy: 0.8318\n",
            "Epoch 151/350\n",
            "391/391 [==============================] - 23s 60ms/step - loss: 0.2241 - accuracy: 0.9198 - val_loss: 0.5559 - val_accuracy: 0.8335\n",
            "Epoch 152/350\n",
            "391/391 [==============================] - 23s 60ms/step - loss: 0.2231 - accuracy: 0.9190 - val_loss: 0.5743 - val_accuracy: 0.8281\n",
            "Epoch 153/350\n",
            "391/391 [==============================] - 23s 60ms/step - loss: 0.2210 - accuracy: 0.9202 - val_loss: 0.5579 - val_accuracy: 0.8337\n",
            "Epoch 154/350\n",
            "391/391 [==============================] - 23s 60ms/step - loss: 0.2215 - accuracy: 0.9197 - val_loss: 0.5626 - val_accuracy: 0.8347\n",
            "Epoch 155/350\n",
            "391/391 [==============================] - 24s 60ms/step - loss: 0.2137 - accuracy: 0.9230 - val_loss: 0.5959 - val_accuracy: 0.8265\n",
            "Epoch 156/350\n",
            "391/391 [==============================] - 23s 60ms/step - loss: 0.2145 - accuracy: 0.9223 - val_loss: 0.5736 - val_accuracy: 0.8323\n",
            "Epoch 157/350\n",
            "391/391 [==============================] - 23s 60ms/step - loss: 0.2149 - accuracy: 0.9238 - val_loss: 0.5941 - val_accuracy: 0.8280\n",
            "Epoch 158/350\n",
            "391/391 [==============================] - 23s 60ms/step - loss: 0.2100 - accuracy: 0.9242 - val_loss: 0.5933 - val_accuracy: 0.8272\n",
            "Epoch 159/350\n",
            "391/391 [==============================] - 23s 60ms/step - loss: 0.2119 - accuracy: 0.9229 - val_loss: 0.5895 - val_accuracy: 0.8250\n",
            "Epoch 160/350\n",
            "391/391 [==============================] - 23s 60ms/step - loss: 0.2047 - accuracy: 0.9262 - val_loss: 0.5806 - val_accuracy: 0.8334\n",
            "Epoch 161/350\n",
            "391/391 [==============================] - 23s 60ms/step - loss: 0.2070 - accuracy: 0.9256 - val_loss: 0.5643 - val_accuracy: 0.8373\n",
            "Epoch 162/350\n",
            "391/391 [==============================] - 23s 60ms/step - loss: 0.2009 - accuracy: 0.9277 - val_loss: 0.5769 - val_accuracy: 0.8346\n",
            "Epoch 163/350\n",
            "391/391 [==============================] - 25s 63ms/step - loss: 0.2007 - accuracy: 0.9281 - val_loss: 0.5792 - val_accuracy: 0.8327\n",
            "Epoch 164/350\n",
            "391/391 [==============================] - 23s 60ms/step - loss: 0.1963 - accuracy: 0.9293 - val_loss: 0.5893 - val_accuracy: 0.8332\n",
            "Epoch 165/350\n",
            "391/391 [==============================] - 23s 60ms/step - loss: 0.1994 - accuracy: 0.9278 - val_loss: 0.6035 - val_accuracy: 0.8288\n",
            "Epoch 166/350\n",
            "391/391 [==============================] - 23s 60ms/step - loss: 0.1956 - accuracy: 0.9293 - val_loss: 0.5949 - val_accuracy: 0.8298\n",
            "Epoch 167/350\n",
            "391/391 [==============================] - 23s 60ms/step - loss: 0.1971 - accuracy: 0.9281 - val_loss: 0.5838 - val_accuracy: 0.8313\n",
            "Epoch 168/350\n",
            "391/391 [==============================] - 23s 60ms/step - loss: 0.1949 - accuracy: 0.9301 - val_loss: 0.6023 - val_accuracy: 0.8278\n",
            "Epoch 169/350\n",
            "391/391 [==============================] - 23s 60ms/step - loss: 0.1962 - accuracy: 0.9295 - val_loss: 0.5943 - val_accuracy: 0.8306\n",
            "Epoch 170/350\n",
            "391/391 [==============================] - 23s 60ms/step - loss: 0.1891 - accuracy: 0.9311 - val_loss: 0.6000 - val_accuracy: 0.8339\n",
            "Epoch 171/350\n",
            "391/391 [==============================] - 23s 60ms/step - loss: 0.1873 - accuracy: 0.9323 - val_loss: 0.5769 - val_accuracy: 0.8359\n",
            "Epoch 172/350\n",
            "391/391 [==============================] - 23s 60ms/step - loss: 0.1856 - accuracy: 0.9334 - val_loss: 0.6093 - val_accuracy: 0.8323\n",
            "Epoch 173/350\n",
            "391/391 [==============================] - 23s 60ms/step - loss: 0.1914 - accuracy: 0.9316 - val_loss: 0.6078 - val_accuracy: 0.8281\n",
            "Epoch 174/350\n",
            "391/391 [==============================] - 23s 60ms/step - loss: 0.1828 - accuracy: 0.9332 - val_loss: 0.5898 - val_accuracy: 0.8324\n",
            "Epoch 175/350\n",
            "391/391 [==============================] - 23s 60ms/step - loss: 0.1838 - accuracy: 0.9342 - val_loss: 0.5964 - val_accuracy: 0.8339\n",
            "Epoch 176/350\n",
            "391/391 [==============================] - 23s 60ms/step - loss: 0.1843 - accuracy: 0.9335 - val_loss: 0.6055 - val_accuracy: 0.8346\n",
            "Epoch 177/350\n",
            "391/391 [==============================] - 25s 63ms/step - loss: 0.1806 - accuracy: 0.9332 - val_loss: 0.6288 - val_accuracy: 0.8325\n",
            "Epoch 178/350\n",
            "391/391 [==============================] - 23s 60ms/step - loss: 0.1812 - accuracy: 0.9341 - val_loss: 0.6011 - val_accuracy: 0.8283\n",
            "Epoch 179/350\n",
            "391/391 [==============================] - 23s 60ms/step - loss: 0.1802 - accuracy: 0.9346 - val_loss: 0.6055 - val_accuracy: 0.8342\n",
            "Epoch 180/350\n",
            "391/391 [==============================] - 23s 60ms/step - loss: 0.1753 - accuracy: 0.9361 - val_loss: 0.6792 - val_accuracy: 0.8221\n",
            "Epoch 181/350\n",
            "391/391 [==============================] - 25s 63ms/step - loss: 0.1752 - accuracy: 0.9351 - val_loss: 0.6056 - val_accuracy: 0.8309\n",
            "Epoch 182/350\n",
            "391/391 [==============================] - 23s 60ms/step - loss: 0.1723 - accuracy: 0.9387 - val_loss: 0.6054 - val_accuracy: 0.8319\n",
            "Epoch 183/350\n",
            "391/391 [==============================] - 23s 60ms/step - loss: 0.1712 - accuracy: 0.9374 - val_loss: 0.6136 - val_accuracy: 0.8291\n",
            "Epoch 184/350\n",
            "391/391 [==============================] - 23s 60ms/step - loss: 0.1719 - accuracy: 0.9374 - val_loss: 0.6005 - val_accuracy: 0.8332\n",
            "Epoch 185/350\n",
            "391/391 [==============================] - 23s 60ms/step - loss: 0.1657 - accuracy: 0.9394 - val_loss: 0.6006 - val_accuracy: 0.8392\n",
            "Epoch 186/350\n",
            "391/391 [==============================] - 23s 60ms/step - loss: 0.1700 - accuracy: 0.9388 - val_loss: 0.5946 - val_accuracy: 0.8353\n",
            "Epoch 187/350\n",
            "391/391 [==============================] - 23s 60ms/step - loss: 0.1692 - accuracy: 0.9382 - val_loss: 0.5928 - val_accuracy: 0.8344\n",
            "Epoch 188/350\n",
            "391/391 [==============================] - 23s 60ms/step - loss: 0.1660 - accuracy: 0.9396 - val_loss: 0.6023 - val_accuracy: 0.8326\n",
            "Epoch 189/350\n",
            "391/391 [==============================] - 23s 60ms/step - loss: 0.1634 - accuracy: 0.9410 - val_loss: 0.6363 - val_accuracy: 0.8311\n",
            "Epoch 190/350\n",
            "391/391 [==============================] - 23s 60ms/step - loss: 0.1580 - accuracy: 0.9422 - val_loss: 0.6121 - val_accuracy: 0.8367\n",
            "Epoch 191/350\n",
            "391/391 [==============================] - 23s 60ms/step - loss: 0.1583 - accuracy: 0.9427 - val_loss: 0.6171 - val_accuracy: 0.8347\n",
            "Epoch 192/350\n",
            "391/391 [==============================] - 23s 60ms/step - loss: 0.1620 - accuracy: 0.9407 - val_loss: 0.5998 - val_accuracy: 0.8384\n",
            "Epoch 193/350\n",
            "391/391 [==============================] - 23s 60ms/step - loss: 0.1612 - accuracy: 0.9412 - val_loss: 0.6293 - val_accuracy: 0.8307\n",
            "Epoch 194/350\n",
            "391/391 [==============================] - 23s 60ms/step - loss: 0.1555 - accuracy: 0.9431 - val_loss: 0.6455 - val_accuracy: 0.8299\n",
            "Epoch 195/350\n",
            "391/391 [==============================] - 23s 60ms/step - loss: 0.1541 - accuracy: 0.9435 - val_loss: 0.6303 - val_accuracy: 0.8333\n",
            "Epoch 196/350\n",
            "391/391 [==============================] - 23s 60ms/step - loss: 0.1578 - accuracy: 0.9422 - val_loss: 0.6302 - val_accuracy: 0.8320\n",
            "Epoch 197/350\n",
            "391/391 [==============================] - 25s 63ms/step - loss: 0.1552 - accuracy: 0.9435 - val_loss: 0.6141 - val_accuracy: 0.8372\n",
            "Epoch 198/350\n",
            "391/391 [==============================] - 23s 60ms/step - loss: 0.1494 - accuracy: 0.9455 - val_loss: 0.6411 - val_accuracy: 0.8310\n",
            "Epoch 199/350\n",
            "391/391 [==============================] - 23s 60ms/step - loss: 0.1528 - accuracy: 0.9434 - val_loss: 0.6261 - val_accuracy: 0.8360\n",
            "Epoch 200/350\n",
            "391/391 [==============================] - 23s 60ms/step - loss: 0.1543 - accuracy: 0.9439 - val_loss: 0.6266 - val_accuracy: 0.8346\n",
            "Epoch 201/350\n",
            "391/391 [==============================] - 24s 60ms/step - loss: 0.1467 - accuracy: 0.9471 - val_loss: 0.6158 - val_accuracy: 0.8340\n",
            "Epoch 202/350\n",
            "391/391 [==============================] - 23s 60ms/step - loss: 0.1461 - accuracy: 0.9468 - val_loss: 0.6173 - val_accuracy: 0.8361\n",
            "Epoch 203/350\n",
            "391/391 [==============================] - 23s 60ms/step - loss: 0.1469 - accuracy: 0.9466 - val_loss: 0.6454 - val_accuracy: 0.8347\n",
            "Epoch 204/350\n",
            "391/391 [==============================] - 23s 60ms/step - loss: 0.1474 - accuracy: 0.9469 - val_loss: 0.6468 - val_accuracy: 0.8351\n",
            "Epoch 205/350\n",
            "391/391 [==============================] - 25s 63ms/step - loss: 0.1450 - accuracy: 0.9478 - val_loss: 0.6547 - val_accuracy: 0.8348\n",
            "Epoch 206/350\n",
            "391/391 [==============================] - 23s 60ms/step - loss: 0.1476 - accuracy: 0.9452 - val_loss: 0.6415 - val_accuracy: 0.8368\n",
            "Epoch 207/350\n",
            "391/391 [==============================] - 23s 60ms/step - loss: 0.1428 - accuracy: 0.9484 - val_loss: 0.6591 - val_accuracy: 0.8341\n",
            "Epoch 208/350\n",
            "391/391 [==============================] - 24s 60ms/step - loss: 0.1430 - accuracy: 0.9465 - val_loss: 0.6095 - val_accuracy: 0.8388\n",
            "Epoch 209/350\n",
            "391/391 [==============================] - 23s 60ms/step - loss: 0.1453 - accuracy: 0.9473 - val_loss: 0.6316 - val_accuracy: 0.8345\n",
            "Epoch 210/350\n",
            "391/391 [==============================] - 25s 63ms/step - loss: 0.1421 - accuracy: 0.9471 - val_loss: 0.6372 - val_accuracy: 0.8346\n",
            "Epoch 211/350\n",
            "391/391 [==============================] - 23s 60ms/step - loss: 0.1411 - accuracy: 0.9485 - val_loss: 0.6579 - val_accuracy: 0.8303\n",
            "Epoch 212/350\n",
            "391/391 [==============================] - 25s 63ms/step - loss: 0.1338 - accuracy: 0.9515 - val_loss: 0.6310 - val_accuracy: 0.8391\n",
            "Epoch 213/350\n",
            "391/391 [==============================] - 23s 60ms/step - loss: 0.1344 - accuracy: 0.9515 - val_loss: 0.6336 - val_accuracy: 0.8356\n",
            "Epoch 214/350\n",
            "391/391 [==============================] - 25s 64ms/step - loss: 0.1346 - accuracy: 0.9502 - val_loss: 0.6901 - val_accuracy: 0.8308\n",
            "Epoch 215/350\n",
            "391/391 [==============================] - 24s 60ms/step - loss: 0.1330 - accuracy: 0.9519 - val_loss: 0.6449 - val_accuracy: 0.8370\n",
            "Epoch 216/350\n",
            "391/391 [==============================] - 25s 63ms/step - loss: 0.1343 - accuracy: 0.9510 - val_loss: 0.6301 - val_accuracy: 0.8379\n",
            "Epoch 217/350\n",
            "391/391 [==============================] - 23s 60ms/step - loss: 0.1346 - accuracy: 0.9521 - val_loss: 0.6285 - val_accuracy: 0.8384\n",
            "Epoch 218/350\n",
            "391/391 [==============================] - 23s 60ms/step - loss: 0.1329 - accuracy: 0.9523 - val_loss: 0.6425 - val_accuracy: 0.8359\n",
            "Epoch 219/350\n",
            "391/391 [==============================] - 23s 60ms/step - loss: 0.1351 - accuracy: 0.9516 - val_loss: 0.6474 - val_accuracy: 0.8379\n",
            "Epoch 220/350\n",
            "391/391 [==============================] - 23s 60ms/step - loss: 0.1325 - accuracy: 0.9516 - val_loss: 0.6413 - val_accuracy: 0.8398\n",
            "Epoch 221/350\n",
            "391/391 [==============================] - 25s 63ms/step - loss: 0.1294 - accuracy: 0.9531 - val_loss: 0.6453 - val_accuracy: 0.8348\n",
            "Epoch 222/350\n",
            "391/391 [==============================] - 24s 60ms/step - loss: 0.1305 - accuracy: 0.9522 - val_loss: 0.6526 - val_accuracy: 0.8369\n",
            "Epoch 223/350\n",
            "391/391 [==============================] - 23s 60ms/step - loss: 0.1271 - accuracy: 0.9546 - val_loss: 0.6566 - val_accuracy: 0.8371\n",
            "Epoch 224/350\n",
            "391/391 [==============================] - 24s 60ms/step - loss: 0.1288 - accuracy: 0.9534 - val_loss: 0.6628 - val_accuracy: 0.8391\n",
            "Epoch 225/350\n",
            "391/391 [==============================] - 25s 63ms/step - loss: 0.1251 - accuracy: 0.9542 - val_loss: 0.6499 - val_accuracy: 0.8381\n",
            "Epoch 226/350\n",
            "391/391 [==============================] - 23s 60ms/step - loss: 0.1212 - accuracy: 0.9563 - val_loss: 0.6898 - val_accuracy: 0.8353\n",
            "Epoch 227/350\n",
            "391/391 [==============================] - 23s 60ms/step - loss: 0.1226 - accuracy: 0.9562 - val_loss: 0.6653 - val_accuracy: 0.8411\n",
            "Epoch 228/350\n",
            "391/391 [==============================] - 23s 60ms/step - loss: 0.1257 - accuracy: 0.9544 - val_loss: 0.6447 - val_accuracy: 0.8384\n",
            "Epoch 229/350\n",
            "391/391 [==============================] - 23s 60ms/step - loss: 0.1239 - accuracy: 0.9542 - val_loss: 0.6435 - val_accuracy: 0.8390\n",
            "Epoch 230/350\n",
            "391/391 [==============================] - 23s 60ms/step - loss: 0.1250 - accuracy: 0.9544 - val_loss: 0.6538 - val_accuracy: 0.8395\n",
            "Epoch 231/350\n",
            "391/391 [==============================] - 25s 63ms/step - loss: 0.1235 - accuracy: 0.9548 - val_loss: 0.6677 - val_accuracy: 0.8350\n",
            "Epoch 232/350\n",
            "391/391 [==============================] - 23s 60ms/step - loss: 0.1228 - accuracy: 0.9545 - val_loss: 0.6592 - val_accuracy: 0.8353\n",
            "Epoch 233/350\n",
            "391/391 [==============================] - 23s 60ms/step - loss: 0.1159 - accuracy: 0.9575 - val_loss: 0.6396 - val_accuracy: 0.8405\n",
            "Epoch 234/350\n",
            "391/391 [==============================] - 23s 60ms/step - loss: 0.1207 - accuracy: 0.9564 - val_loss: 0.6493 - val_accuracy: 0.8394\n",
            "Epoch 235/350\n",
            "391/391 [==============================] - 23s 60ms/step - loss: 0.1198 - accuracy: 0.9569 - val_loss: 0.6353 - val_accuracy: 0.8412\n",
            "Epoch 236/350\n",
            "391/391 [==============================] - 23s 60ms/step - loss: 0.1150 - accuracy: 0.9578 - val_loss: 0.6649 - val_accuracy: 0.8388\n",
            "Epoch 237/350\n",
            "391/391 [==============================] - 23s 60ms/step - loss: 0.1172 - accuracy: 0.9577 - val_loss: 0.6463 - val_accuracy: 0.8398\n",
            "Epoch 238/350\n",
            "391/391 [==============================] - 25s 63ms/step - loss: 0.1112 - accuracy: 0.9608 - val_loss: 0.6734 - val_accuracy: 0.8406\n",
            "Epoch 239/350\n",
            "391/391 [==============================] - 24s 60ms/step - loss: 0.1149 - accuracy: 0.9577 - val_loss: 0.6777 - val_accuracy: 0.8366\n",
            "Epoch 240/350\n",
            "391/391 [==============================] - 25s 63ms/step - loss: 0.1156 - accuracy: 0.9584 - val_loss: 0.6961 - val_accuracy: 0.8355\n",
            "Epoch 241/350\n",
            "391/391 [==============================] - 23s 60ms/step - loss: 0.1131 - accuracy: 0.9583 - val_loss: 0.6670 - val_accuracy: 0.8407\n",
            "Epoch 242/350\n",
            "391/391 [==============================] - 23s 60ms/step - loss: 0.1131 - accuracy: 0.9595 - val_loss: 0.6631 - val_accuracy: 0.8374\n",
            "Epoch 243/350\n",
            "391/391 [==============================] - 23s 60ms/step - loss: 0.1132 - accuracy: 0.9591 - val_loss: 0.6672 - val_accuracy: 0.8383\n",
            "Epoch 244/350\n",
            "391/391 [==============================] - 23s 60ms/step - loss: 0.1100 - accuracy: 0.9608 - val_loss: 0.6752 - val_accuracy: 0.8338\n",
            "Epoch 245/350\n",
            "391/391 [==============================] - 25s 63ms/step - loss: 0.1132 - accuracy: 0.9586 - val_loss: 0.6681 - val_accuracy: 0.8385\n",
            "Epoch 246/350\n",
            "391/391 [==============================] - 23s 60ms/step - loss: 0.1096 - accuracy: 0.9609 - val_loss: 0.6771 - val_accuracy: 0.8322\n",
            "Epoch 247/350\n",
            "391/391 [==============================] - 24s 60ms/step - loss: 0.1090 - accuracy: 0.9606 - val_loss: 0.6751 - val_accuracy: 0.8362\n",
            "Epoch 248/350\n",
            "391/391 [==============================] - 23s 60ms/step - loss: 0.1147 - accuracy: 0.9589 - val_loss: 0.6532 - val_accuracy: 0.8353\n",
            "Epoch 249/350\n",
            "391/391 [==============================] - 24s 60ms/step - loss: 0.1057 - accuracy: 0.9619 - val_loss: 0.6953 - val_accuracy: 0.8369\n",
            "Epoch 250/350\n",
            "391/391 [==============================] - 23s 60ms/step - loss: 0.1100 - accuracy: 0.9608 - val_loss: 0.6634 - val_accuracy: 0.8400\n",
            "Epoch 251/350\n",
            "391/391 [==============================] - 24s 60ms/step - loss: 0.1091 - accuracy: 0.9601 - val_loss: 0.6725 - val_accuracy: 0.8376\n",
            "Epoch 252/350\n",
            "391/391 [==============================] - 23s 60ms/step - loss: 0.1097 - accuracy: 0.9599 - val_loss: 0.6417 - val_accuracy: 0.8405\n",
            "Epoch 253/350\n",
            "391/391 [==============================] - 23s 60ms/step - loss: 0.1058 - accuracy: 0.9616 - val_loss: 0.6822 - val_accuracy: 0.8394\n",
            "Epoch 254/350\n",
            "391/391 [==============================] - 24s 60ms/step - loss: 0.1061 - accuracy: 0.9609 - val_loss: 0.6977 - val_accuracy: 0.8368\n",
            "Epoch 255/350\n",
            "391/391 [==============================] - 25s 63ms/step - loss: 0.1087 - accuracy: 0.9606 - val_loss: 0.6667 - val_accuracy: 0.8388\n",
            "Epoch 256/350\n",
            "391/391 [==============================] - 24s 60ms/step - loss: 0.1088 - accuracy: 0.9607 - val_loss: 0.7037 - val_accuracy: 0.8305\n",
            "Epoch 257/350\n",
            "391/391 [==============================] - 24s 60ms/step - loss: 0.1021 - accuracy: 0.9622 - val_loss: 0.6856 - val_accuracy: 0.8380\n",
            "Epoch 258/350\n",
            "391/391 [==============================] - 24s 60ms/step - loss: 0.1034 - accuracy: 0.9627 - val_loss: 0.7381 - val_accuracy: 0.8315\n",
            "Epoch 259/350\n",
            "391/391 [==============================] - 24s 61ms/step - loss: 0.1061 - accuracy: 0.9616 - val_loss: 0.6860 - val_accuracy: 0.8398\n",
            "Epoch 260/350\n",
            "391/391 [==============================] - 24s 60ms/step - loss: 0.1002 - accuracy: 0.9652 - val_loss: 0.6651 - val_accuracy: 0.8398\n",
            "Epoch 261/350\n",
            "391/391 [==============================] - 24s 60ms/step - loss: 0.1022 - accuracy: 0.9633 - val_loss: 0.6697 - val_accuracy: 0.8399\n",
            "Epoch 262/350\n",
            "391/391 [==============================] - 24s 60ms/step - loss: 0.1062 - accuracy: 0.9614 - val_loss: 0.7147 - val_accuracy: 0.8313\n",
            "Epoch 263/350\n",
            "391/391 [==============================] - 23s 60ms/step - loss: 0.1001 - accuracy: 0.9639 - val_loss: 0.6887 - val_accuracy: 0.8391\n",
            "Epoch 264/350\n",
            "391/391 [==============================] - 24s 61ms/step - loss: 0.1058 - accuracy: 0.9613 - val_loss: 0.7274 - val_accuracy: 0.8332\n",
            "Epoch 265/350\n",
            "391/391 [==============================] - 24s 60ms/step - loss: 0.1033 - accuracy: 0.9625 - val_loss: 0.6775 - val_accuracy: 0.8406\n",
            "Epoch 266/350\n",
            "391/391 [==============================] - 24s 61ms/step - loss: 0.0975 - accuracy: 0.9650 - val_loss: 0.6689 - val_accuracy: 0.8416\n",
            "Epoch 267/350\n",
            "391/391 [==============================] - 24s 60ms/step - loss: 0.0967 - accuracy: 0.9648 - val_loss: 0.6809 - val_accuracy: 0.8424\n",
            "Epoch 268/350\n",
            "391/391 [==============================] - 24s 60ms/step - loss: 0.0996 - accuracy: 0.9636 - val_loss: 0.6711 - val_accuracy: 0.8411\n",
            "Epoch 269/350\n",
            "391/391 [==============================] - 23s 60ms/step - loss: 0.0968 - accuracy: 0.9652 - val_loss: 0.6713 - val_accuracy: 0.8426\n",
            "Epoch 270/350\n",
            "391/391 [==============================] - 24s 60ms/step - loss: 0.0973 - accuracy: 0.9651 - val_loss: 0.7408 - val_accuracy: 0.8354\n",
            "Epoch 271/350\n",
            "391/391 [==============================] - 24s 60ms/step - loss: 0.0987 - accuracy: 0.9646 - val_loss: 0.6731 - val_accuracy: 0.8406\n",
            "Epoch 272/350\n",
            "391/391 [==============================] - 24s 60ms/step - loss: 0.0973 - accuracy: 0.9651 - val_loss: 0.6884 - val_accuracy: 0.8406\n",
            "Epoch 273/350\n",
            "391/391 [==============================] - 24s 61ms/step - loss: 0.0953 - accuracy: 0.9656 - val_loss: 0.7186 - val_accuracy: 0.8369\n",
            "Epoch 274/350\n",
            "391/391 [==============================] - 24s 60ms/step - loss: 0.0977 - accuracy: 0.9647 - val_loss: 0.6889 - val_accuracy: 0.8391\n",
            "Epoch 275/350\n",
            "391/391 [==============================] - 24s 60ms/step - loss: 0.0992 - accuracy: 0.9649 - val_loss: 0.6738 - val_accuracy: 0.8411\n",
            "Epoch 276/350\n",
            "391/391 [==============================] - 24s 60ms/step - loss: 0.0953 - accuracy: 0.9661 - val_loss: 0.6824 - val_accuracy: 0.8403\n",
            "Epoch 277/350\n",
            "391/391 [==============================] - 24s 61ms/step - loss: 0.0931 - accuracy: 0.9660 - val_loss: 0.7463 - val_accuracy: 0.8355\n",
            "Epoch 278/350\n",
            "391/391 [==============================] - 23s 60ms/step - loss: 0.0944 - accuracy: 0.9655 - val_loss: 0.6981 - val_accuracy: 0.8403\n",
            "Epoch 279/350\n",
            "391/391 [==============================] - 24s 60ms/step - loss: 0.0946 - accuracy: 0.9666 - val_loss: 0.7062 - val_accuracy: 0.8419\n",
            "Epoch 280/350\n",
            "391/391 [==============================] - 24s 60ms/step - loss: 0.0932 - accuracy: 0.9666 - val_loss: 0.7488 - val_accuracy: 0.8314\n",
            "Epoch 281/350\n",
            "391/391 [==============================] - 24s 61ms/step - loss: 0.0890 - accuracy: 0.9683 - val_loss: 0.6818 - val_accuracy: 0.8416\n",
            "Epoch 282/350\n",
            "391/391 [==============================] - 24s 60ms/step - loss: 0.0891 - accuracy: 0.9678 - val_loss: 0.6854 - val_accuracy: 0.8410\n",
            "Epoch 283/350\n",
            "391/391 [==============================] - 24s 61ms/step - loss: 0.0906 - accuracy: 0.9677 - val_loss: 0.7246 - val_accuracy: 0.8352\n",
            "Epoch 284/350\n",
            "391/391 [==============================] - 23s 60ms/step - loss: 0.0916 - accuracy: 0.9663 - val_loss: 0.6925 - val_accuracy: 0.8384\n",
            "Epoch 285/350\n",
            "391/391 [==============================] - 24s 60ms/step - loss: 0.0900 - accuracy: 0.9674 - val_loss: 0.6867 - val_accuracy: 0.8402\n",
            "Epoch 286/350\n",
            "391/391 [==============================] - 24s 60ms/step - loss: 0.0910 - accuracy: 0.9676 - val_loss: 0.7020 - val_accuracy: 0.8404\n",
            "Epoch 287/350\n",
            "391/391 [==============================] - 24s 60ms/step - loss: 0.0903 - accuracy: 0.9682 - val_loss: 0.7183 - val_accuracy: 0.8409\n",
            "Epoch 288/350\n",
            "391/391 [==============================] - 24s 61ms/step - loss: 0.0887 - accuracy: 0.9678 - val_loss: 0.7061 - val_accuracy: 0.8405\n",
            "Epoch 289/350\n",
            "391/391 [==============================] - 24s 60ms/step - loss: 0.0899 - accuracy: 0.9671 - val_loss: 0.7058 - val_accuracy: 0.8401\n",
            "Epoch 290/350\n",
            "391/391 [==============================] - 24s 60ms/step - loss: 0.0913 - accuracy: 0.9664 - val_loss: 0.7138 - val_accuracy: 0.8393\n",
            "Epoch 291/350\n",
            "391/391 [==============================] - 24s 60ms/step - loss: 0.0846 - accuracy: 0.9690 - val_loss: 0.7232 - val_accuracy: 0.8391\n",
            "Epoch 292/350\n",
            "391/391 [==============================] - 24s 60ms/step - loss: 0.0915 - accuracy: 0.9666 - val_loss: 0.6925 - val_accuracy: 0.8442\n",
            "Epoch 293/350\n",
            "391/391 [==============================] - 24s 60ms/step - loss: 0.0855 - accuracy: 0.9687 - val_loss: 0.7081 - val_accuracy: 0.8427\n",
            "Epoch 294/350\n",
            "391/391 [==============================] - 24s 60ms/step - loss: 0.0840 - accuracy: 0.9693 - val_loss: 0.6984 - val_accuracy: 0.8446\n",
            "Epoch 295/350\n",
            "391/391 [==============================] - 24s 60ms/step - loss: 0.0878 - accuracy: 0.9693 - val_loss: 0.7012 - val_accuracy: 0.8448\n",
            "Epoch 296/350\n",
            "391/391 [==============================] - 24s 60ms/step - loss: 0.0825 - accuracy: 0.9698 - val_loss: 0.7012 - val_accuracy: 0.8428\n",
            "Epoch 297/350\n",
            "391/391 [==============================] - 24s 60ms/step - loss: 0.0859 - accuracy: 0.9701 - val_loss: 0.6913 - val_accuracy: 0.8449\n",
            "Epoch 298/350\n",
            "391/391 [==============================] - 24s 61ms/step - loss: 0.0867 - accuracy: 0.9699 - val_loss: 0.7255 - val_accuracy: 0.8380\n",
            "Epoch 299/350\n",
            "391/391 [==============================] - 24s 60ms/step - loss: 0.0819 - accuracy: 0.9700 - val_loss: 0.7336 - val_accuracy: 0.8378\n",
            "Epoch 300/350\n",
            "391/391 [==============================] - 24s 61ms/step - loss: 0.0848 - accuracy: 0.9693 - val_loss: 0.7376 - val_accuracy: 0.8351\n",
            "Epoch 301/350\n",
            "391/391 [==============================] - 24s 60ms/step - loss: 0.0874 - accuracy: 0.9687 - val_loss: 0.7087 - val_accuracy: 0.8404\n",
            "Epoch 302/350\n",
            "391/391 [==============================] - 25s 64ms/step - loss: 0.0878 - accuracy: 0.9682 - val_loss: 0.6758 - val_accuracy: 0.8460\n",
            "Epoch 303/350\n",
            "391/391 [==============================] - 24s 60ms/step - loss: 0.0784 - accuracy: 0.9718 - val_loss: 0.7151 - val_accuracy: 0.8391\n",
            "Epoch 304/350\n",
            "391/391 [==============================] - 24s 61ms/step - loss: 0.0817 - accuracy: 0.9707 - val_loss: 0.7267 - val_accuracy: 0.8437\n",
            "Epoch 305/350\n",
            "391/391 [==============================] - 24s 60ms/step - loss: 0.0824 - accuracy: 0.9699 - val_loss: 0.7085 - val_accuracy: 0.8436\n",
            "Epoch 306/350\n",
            "391/391 [==============================] - 24s 61ms/step - loss: 0.0809 - accuracy: 0.9710 - val_loss: 0.7108 - val_accuracy: 0.8394\n",
            "Epoch 307/350\n",
            "391/391 [==============================] - 24s 60ms/step - loss: 0.0772 - accuracy: 0.9722 - val_loss: 0.7326 - val_accuracy: 0.8397\n",
            "Epoch 308/350\n",
            "391/391 [==============================] - 24s 61ms/step - loss: 0.0812 - accuracy: 0.9706 - val_loss: 0.7735 - val_accuracy: 0.8426\n",
            "Epoch 309/350\n",
            "391/391 [==============================] - 23s 60ms/step - loss: 0.0803 - accuracy: 0.9706 - val_loss: 0.7357 - val_accuracy: 0.8403\n",
            "Epoch 310/350\n",
            "391/391 [==============================] - 24s 60ms/step - loss: 0.0793 - accuracy: 0.9723 - val_loss: 0.7437 - val_accuracy: 0.8397\n",
            "Epoch 311/350\n",
            "391/391 [==============================] - 25s 63ms/step - loss: 0.0817 - accuracy: 0.9709 - val_loss: 0.7171 - val_accuracy: 0.8413\n",
            "Epoch 312/350\n",
            "391/391 [==============================] - 25s 63ms/step - loss: 0.0835 - accuracy: 0.9694 - val_loss: 0.6813 - val_accuracy: 0.8426\n",
            "Epoch 313/350\n",
            "391/391 [==============================] - 24s 60ms/step - loss: 0.0784 - accuracy: 0.9719 - val_loss: 0.7011 - val_accuracy: 0.8424\n",
            "Epoch 314/350\n",
            "391/391 [==============================] - 24s 61ms/step - loss: 0.0793 - accuracy: 0.9711 - val_loss: 0.7015 - val_accuracy: 0.8422\n",
            "Epoch 315/350\n",
            "391/391 [==============================] - 24s 60ms/step - loss: 0.0848 - accuracy: 0.9691 - val_loss: 0.7213 - val_accuracy: 0.8360\n",
            "Epoch 316/350\n",
            "391/391 [==============================] - 24s 60ms/step - loss: 0.0778 - accuracy: 0.9718 - val_loss: 0.7412 - val_accuracy: 0.8381\n",
            "Epoch 317/350\n",
            "391/391 [==============================] - 25s 63ms/step - loss: 0.0768 - accuracy: 0.9724 - val_loss: 0.7179 - val_accuracy: 0.8414\n",
            "Epoch 318/350\n",
            "391/391 [==============================] - 24s 60ms/step - loss: 0.0771 - accuracy: 0.9722 - val_loss: 0.7032 - val_accuracy: 0.8434\n",
            "Epoch 319/350\n",
            "391/391 [==============================] - 24s 60ms/step - loss: 0.0766 - accuracy: 0.9727 - val_loss: 0.7024 - val_accuracy: 0.8387\n",
            "Epoch 320/350\n",
            "391/391 [==============================] - 25s 63ms/step - loss: 0.0771 - accuracy: 0.9724 - val_loss: 0.7194 - val_accuracy: 0.8446\n",
            "Epoch 321/350\n",
            "391/391 [==============================] - 24s 61ms/step - loss: 0.0763 - accuracy: 0.9719 - val_loss: 0.7192 - val_accuracy: 0.8431\n",
            "Epoch 322/350\n",
            "391/391 [==============================] - 24s 60ms/step - loss: 0.0754 - accuracy: 0.9724 - val_loss: 0.7338 - val_accuracy: 0.8374\n",
            "Epoch 323/350\n",
            "391/391 [==============================] - 24s 61ms/step - loss: 0.0787 - accuracy: 0.9718 - val_loss: 0.7202 - val_accuracy: 0.8437\n",
            "Epoch 324/350\n",
            "391/391 [==============================] - 23s 60ms/step - loss: 0.0784 - accuracy: 0.9717 - val_loss: 0.7208 - val_accuracy: 0.8428\n",
            "Epoch 325/350\n",
            "391/391 [==============================] - 24s 60ms/step - loss: 0.0761 - accuracy: 0.9729 - val_loss: 0.7777 - val_accuracy: 0.8366\n",
            "Epoch 326/350\n",
            "391/391 [==============================] - 23s 60ms/step - loss: 0.0745 - accuracy: 0.9730 - val_loss: 0.7348 - val_accuracy: 0.8437\n",
            "Epoch 327/350\n",
            "391/391 [==============================] - 24s 61ms/step - loss: 0.0759 - accuracy: 0.9736 - val_loss: 0.7502 - val_accuracy: 0.8383\n",
            "Epoch 328/350\n",
            "391/391 [==============================] - 23s 60ms/step - loss: 0.0740 - accuracy: 0.9736 - val_loss: 0.7323 - val_accuracy: 0.8417\n",
            "Epoch 329/350\n",
            "391/391 [==============================] - 24s 60ms/step - loss: 0.0744 - accuracy: 0.9727 - val_loss: 0.7743 - val_accuracy: 0.8367\n",
            "Epoch 330/350\n",
            "391/391 [==============================] - 24s 60ms/step - loss: 0.0718 - accuracy: 0.9736 - val_loss: 0.7638 - val_accuracy: 0.8414\n",
            "Epoch 331/350\n",
            "391/391 [==============================] - 24s 61ms/step - loss: 0.0757 - accuracy: 0.9723 - val_loss: 0.7089 - val_accuracy: 0.8432\n",
            "Epoch 332/350\n",
            "391/391 [==============================] - 24s 60ms/step - loss: 0.0715 - accuracy: 0.9742 - val_loss: 0.7120 - val_accuracy: 0.8445\n",
            "Epoch 333/350\n",
            "391/391 [==============================] - 24s 61ms/step - loss: 0.0722 - accuracy: 0.9748 - val_loss: 0.7469 - val_accuracy: 0.8406\n",
            "Epoch 334/350\n",
            "391/391 [==============================] - 24s 60ms/step - loss: 0.0727 - accuracy: 0.9737 - val_loss: 0.7338 - val_accuracy: 0.8441\n",
            "Epoch 335/350\n",
            "391/391 [==============================] - 24s 60ms/step - loss: 0.0692 - accuracy: 0.9753 - val_loss: 0.7205 - val_accuracy: 0.8440\n",
            "Epoch 336/350\n",
            "391/391 [==============================] - 24s 60ms/step - loss: 0.0730 - accuracy: 0.9744 - val_loss: 0.7111 - val_accuracy: 0.8469\n",
            "Epoch 337/350\n",
            "391/391 [==============================] - 24s 60ms/step - loss: 0.0706 - accuracy: 0.9749 - val_loss: 0.7428 - val_accuracy: 0.8407\n",
            "Epoch 338/350\n",
            "391/391 [==============================] - 24s 60ms/step - loss: 0.0760 - accuracy: 0.9721 - val_loss: 0.7034 - val_accuracy: 0.8447\n",
            "Epoch 339/350\n",
            "391/391 [==============================] - 24s 60ms/step - loss: 0.0712 - accuracy: 0.9742 - val_loss: 0.7397 - val_accuracy: 0.8418\n",
            "Epoch 340/350\n",
            "391/391 [==============================] - 24s 60ms/step - loss: 0.0670 - accuracy: 0.9765 - val_loss: 0.7194 - val_accuracy: 0.8419\n",
            "Epoch 341/350\n",
            "391/391 [==============================] - 25s 64ms/step - loss: 0.0715 - accuracy: 0.9741 - val_loss: 0.7392 - val_accuracy: 0.8415\n",
            "Epoch 342/350\n",
            "391/391 [==============================] - 23s 60ms/step - loss: 0.0682 - accuracy: 0.9751 - val_loss: 0.7229 - val_accuracy: 0.8443\n",
            "Epoch 343/350\n",
            "391/391 [==============================] - 24s 60ms/step - loss: 0.0705 - accuracy: 0.9745 - val_loss: 0.7588 - val_accuracy: 0.8421\n",
            "Epoch 344/350\n",
            "391/391 [==============================] - 24s 61ms/step - loss: 0.0735 - accuracy: 0.9735 - val_loss: 0.7269 - val_accuracy: 0.8396\n",
            "Epoch 345/350\n",
            "391/391 [==============================] - 24s 61ms/step - loss: 0.0690 - accuracy: 0.9745 - val_loss: 0.7375 - val_accuracy: 0.8464\n",
            "Epoch 346/350\n",
            "391/391 [==============================] - 24s 61ms/step - loss: 0.0702 - accuracy: 0.9754 - val_loss: 0.7433 - val_accuracy: 0.8424\n",
            "Epoch 347/350\n",
            "391/391 [==============================] - 23s 60ms/step - loss: 0.0673 - accuracy: 0.9765 - val_loss: 0.7485 - val_accuracy: 0.8395\n",
            "Epoch 348/350\n",
            "391/391 [==============================] - 24s 61ms/step - loss: 0.0686 - accuracy: 0.9760 - val_loss: 0.7241 - val_accuracy: 0.8482\n",
            "Epoch 349/350\n",
            "391/391 [==============================] - 24s 60ms/step - loss: 0.0705 - accuracy: 0.9749 - val_loss: 0.7614 - val_accuracy: 0.8373\n",
            "Epoch 350/350\n",
            "391/391 [==============================] - 24s 60ms/step - loss: 0.0678 - accuracy: 0.9754 - val_loss: 0.7854 - val_accuracy: 0.8368\n"
          ]
        }
      ],
      "source": [
        "model_details = model.fit(X_train, Y_train_en,\n",
        "                    batch_size = 128,\n",
        "                    epochs = NUM_EPOCH, # number of iterations\n",
        "                    validation_data= (X_test, Y_test_en),\n",
        "                    callbacks=[checkpoint],\n",
        "                    verbose=1)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vwJ0AOYr11XD",
        "outputId": "d71e2c6c-2ff7-4075-91e4-3f1d795d46e9"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Accuracy: 83.68%\n"
          ]
        }
      ],
      "source": [
        "scores = model.evaluate(X_test, Y_test_en, verbose=0)\n",
        "print(\"Accuracy: %.2f%%\" % (scores[1]*100))\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "B32cn5xG2Ds6"
      },
      "outputs": [],
      "source": [
        "plot_model(model_details)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "tJsRnZ4L3AQo"
      },
      "outputs": [],
      "source": [
        "datagen = ImageDataGenerator(\n",
        "    featurewise_center=False,  # set input mean to 0 over the dataset\n",
        "    samplewise_center=False,  # set each sample mean to 0\n",
        "    featurewise_std_normalization=False,  # divide inputs by std of the dataset\n",
        "    samplewise_std_normalization=False,  # divide each input by its std\n",
        "    zca_whitening=False,  # apply ZCA whitening\n",
        "    rotation_range=45,  # randomly rotate images in the range (degrees, 0 to 180)\n",
        "    width_shift_range=0.2,  # randomly shift images horizontally (fraction of total width)\n",
        "    height_shift_range=0.2,  # randomly shift images vertically (fraction of total height)\n",
        "    horizontal_flip=True,  # randomly flip images\n",
        "    vertical_flip=False)  # randomly flip images\n",
        "\n",
        "datagen.fit(X_train)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3zJfuyzo3ZO6",
        "outputId": "ea4a2fc4-0aed-4d64-9758-ff62a1a63609"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Model: \"sequential_1\"\n",
            "_________________________________________________________________\n",
            " Layer (type)                Output Shape              Param #   \n",
            "=================================================================\n",
            " conv2d_9 (Conv2D)           (None, 32, 32, 96)        2688      \n",
            "                                                                 \n",
            " dropout_3 (Dropout)         (None, 32, 32, 96)        0         \n",
            "                                                                 \n",
            " conv2d_10 (Conv2D)          (None, 32, 32, 96)        83040     \n",
            "                                                                 \n",
            " conv2d_11 (Conv2D)          (None, 16, 16, 96)        83040     \n",
            "                                                                 \n",
            " dropout_4 (Dropout)         (None, 16, 16, 96)        0         \n",
            "                                                                 \n",
            " conv2d_12 (Conv2D)          (None, 16, 16, 192)       166080    \n",
            "                                                                 \n",
            " conv2d_13 (Conv2D)          (None, 16, 16, 192)       331968    \n",
            "                                                                 \n",
            " conv2d_14 (Conv2D)          (None, 8, 8, 192)         331968    \n",
            "                                                                 \n",
            " dropout_5 (Dropout)         (None, 8, 8, 192)         0         \n",
            "                                                                 \n",
            " conv2d_15 (Conv2D)          (None, 8, 8, 192)         331968    \n",
            "                                                                 \n",
            " activation_3 (Activation)   (None, 8, 8, 192)         0         \n",
            "                                                                 \n",
            " conv2d_16 (Conv2D)          (None, 8, 8, 192)         37056     \n",
            "                                                                 \n",
            " activation_4 (Activation)   (None, 8, 8, 192)         0         \n",
            "                                                                 \n",
            " conv2d_17 (Conv2D)          (None, 8, 8, 10)          1930      \n",
            "                                                                 \n",
            " global_average_pooling2d_1   (None, 10)               0         \n",
            " (GlobalAveragePooling2D)                                        \n",
            "                                                                 \n",
            " activation_5 (Activation)   (None, 10)                0         \n",
            "                                                                 \n",
            "=================================================================\n",
            "Total params: 1,369,738\n",
            "Trainable params: 1,369,738\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n"
          ]
        }
      ],
      "source": [
        "augmented_model = pure_cnn_model()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "IqumKptP3bhP"
      },
      "outputs": [],
      "source": [
        "augmented_checkpoint = ModelCheckpoint('augmented_best_model.h5',  # model filename\n",
        "                             monitor='val_loss', # quantity to monitor\n",
        "                             verbose=0, # verbosity - 0 or 1\n",
        "                             save_best_only= True, # The latest best model will not be overwritten\n",
        "                             mode='auto') # The decision to overwrite model is made \n",
        "                                          # automatically depending on the quantity to monitor "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "rPAWo1wW3diF"
      },
      "outputs": [],
      "source": [
        "augmented_model.compile(loss='categorical_crossentropy', # Better loss function for neural networks\n",
        "              optimizer=Adam(lr=LEARN_RATE), # Adam optimizer with 1.0e-4 learning rate\n",
        "              metrics = ['accuracy']) # Metrics to be evaluated by the model\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 449
        },
        "id": "u3nkzDS93fX5",
        "outputId": "de498dc6-8727-4225-a8ed-b73adde3f2e3"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "<ipython-input-15-699b76579406>:1: UserWarning: `Model.fit_generator` is deprecated and will be removed in a future version. Please use `Model.fit`, which supports generators.\n",
            "  augmented_model_details = augmented_model.fit_generator(datagen.flow(X_train, Y_train_en, batch_size = 32),\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 1/350\n",
            " 420/1562 [=======>......................] - ETA: 24:31 - loss: 2.1230 - accuracy: 0.1835"
          ]
        },
        {
          "ename": "KeyboardInterrupt",
          "evalue": "ignored",
          "output_type": "error",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-15-699b76579406>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m augmented_model_details = augmented_model.fit_generator(datagen.flow(X_train, Y_train_en, batch_size = 32),\n\u001b[0m\u001b[1;32m      2\u001b[0m                     \u001b[0msteps_per_epoch\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_train\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m/\u001b[0m \u001b[0;36m32\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;31m# number of samples per gradient update\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m                     \u001b[0mepochs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mNUM_EPOCH\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;31m# number of iterations\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m                     \u001b[0mvalidation_data\u001b[0m\u001b[0;34m=\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mX_test\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mY_test_en\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m                     \u001b[0mcallbacks\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0maugmented_checkpoint\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.8/dist-packages/keras/engine/training.py\u001b[0m in \u001b[0;36mfit_generator\u001b[0;34m(self, generator, steps_per_epoch, epochs, verbose, callbacks, validation_data, validation_steps, validation_freq, class_weight, max_queue_size, workers, use_multiprocessing, shuffle, initial_epoch)\u001b[0m\n\u001b[1;32m   2258\u001b[0m         \u001b[0;34m'Please use `Model.fit`, which supports generators.'\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2259\u001b[0m         stacklevel=2)\n\u001b[0;32m-> 2260\u001b[0;31m     return self.fit(\n\u001b[0m\u001b[1;32m   2261\u001b[0m         \u001b[0mgenerator\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2262\u001b[0m         \u001b[0msteps_per_epoch\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0msteps_per_epoch\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.8/dist-packages/keras/utils/traceback_utils.py\u001b[0m in \u001b[0;36merror_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     62\u001b[0m     \u001b[0mfiltered_tb\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     63\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 64\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     65\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0mException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m  \u001b[0;31m# pylint: disable=broad-except\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     66\u001b[0m       \u001b[0mfiltered_tb\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_process_traceback_frames\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__traceback__\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.8/dist-packages/keras/engine/training.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, validation_batch_size, validation_freq, max_queue_size, workers, use_multiprocessing)\u001b[0m\n\u001b[1;32m   1407\u001b[0m                 _r=1):\n\u001b[1;32m   1408\u001b[0m               \u001b[0mcallbacks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mon_train_batch_begin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1409\u001b[0;31m               \u001b[0mtmp_logs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0miterator\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1410\u001b[0m               \u001b[0;32mif\u001b[0m \u001b[0mdata_handler\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshould_sync\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1411\u001b[0m                 \u001b[0mcontext\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0masync_wait\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.8/dist-packages/tensorflow/python/util/traceback_utils.py\u001b[0m in \u001b[0;36merror_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    148\u001b[0m     \u001b[0mfiltered_tb\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    149\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 150\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    151\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0mException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    152\u001b[0m       \u001b[0mfiltered_tb\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_process_traceback_frames\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__traceback__\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.8/dist-packages/tensorflow/python/eager/def_function.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args, **kwds)\u001b[0m\n\u001b[1;32m    913\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    914\u001b[0m       \u001b[0;32mwith\u001b[0m \u001b[0mOptionalXlaContext\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_jit_compile\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 915\u001b[0;31m         \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    916\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    917\u001b[0m       \u001b[0mnew_tracing_count\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexperimental_get_tracing_count\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.8/dist-packages/tensorflow/python/eager/def_function.py\u001b[0m in \u001b[0;36m_call\u001b[0;34m(self, *args, **kwds)\u001b[0m\n\u001b[1;32m    945\u001b[0m       \u001b[0;31m# In this case we have created variables on the first call, so we run the\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    946\u001b[0m       \u001b[0;31m# defunned version which is guaranteed to never create variables.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 947\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_stateless_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# pylint: disable=not-callable\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    948\u001b[0m     \u001b[0;32melif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_stateful_fn\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    949\u001b[0m       \u001b[0;31m# Release the lock early so that multiple threads can perform the call\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.8/dist-packages/tensorflow/python/eager/function.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   2451\u001b[0m       (graph_function,\n\u001b[1;32m   2452\u001b[0m        filtered_flat_args) = self._maybe_define_function(args, kwargs)\n\u001b[0;32m-> 2453\u001b[0;31m     return graph_function._call_flat(\n\u001b[0m\u001b[1;32m   2454\u001b[0m         filtered_flat_args, captured_inputs=graph_function.captured_inputs)  # pylint: disable=protected-access\n\u001b[1;32m   2455\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.8/dist-packages/tensorflow/python/eager/function.py\u001b[0m in \u001b[0;36m_call_flat\u001b[0;34m(self, args, captured_inputs, cancellation_manager)\u001b[0m\n\u001b[1;32m   1858\u001b[0m         and executing_eagerly):\n\u001b[1;32m   1859\u001b[0m       \u001b[0;31m# No tape is watching; skip to running the function.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1860\u001b[0;31m       return self._build_call_outputs(self._inference_function.call(\n\u001b[0m\u001b[1;32m   1861\u001b[0m           ctx, args, cancellation_manager=cancellation_manager))\n\u001b[1;32m   1862\u001b[0m     forward_backward = self._select_forward_and_backward_functions(\n",
            "\u001b[0;32m/usr/local/lib/python3.8/dist-packages/tensorflow/python/eager/function.py\u001b[0m in \u001b[0;36mcall\u001b[0;34m(self, ctx, args, cancellation_manager)\u001b[0m\n\u001b[1;32m    495\u001b[0m       \u001b[0;32mwith\u001b[0m \u001b[0m_InterpolateFunctionError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    496\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mcancellation_manager\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 497\u001b[0;31m           outputs = execute.execute(\n\u001b[0m\u001b[1;32m    498\u001b[0m               \u001b[0mstr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msignature\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    499\u001b[0m               \u001b[0mnum_outputs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_num_outputs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.8/dist-packages/tensorflow/python/eager/execute.py\u001b[0m in \u001b[0;36mquick_execute\u001b[0;34m(op_name, num_outputs, inputs, attrs, ctx, name)\u001b[0m\n\u001b[1;32m     52\u001b[0m   \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     53\u001b[0m     \u001b[0mctx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mensure_initialized\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 54\u001b[0;31m     tensors = pywrap_tfe.TFE_Py_Execute(ctx._handle, device_name, op_name,\n\u001b[0m\u001b[1;32m     55\u001b[0m                                         inputs, attrs, num_outputs)\n\u001b[1;32m     56\u001b[0m   \u001b[0;32mexcept\u001b[0m \u001b[0mcore\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_NotOkStatusException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ],
      "source": [
        "augmented_model_details = augmented_model.fit_generator(datagen.flow(X_train, Y_train_en, batch_size = 32),\n",
        "                    steps_per_epoch = len(X_train) / 32, # number of samples per gradient update\n",
        "                    epochs = NUM_EPOCH, # number of iterations\n",
        "                    validation_data= (X_test, Y_test_en),\n",
        "                    callbacks=[augmented_checkpoint],\n",
        "                    verbose=1)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "btPuraIV3r7q"
      },
      "outputs": [],
      "source": [
        "scores = augmented_model.evaluate(X_test, Y_test_en, verbose=0)\n",
        "print(\"Accuracy: %.2f%%\" % (scores[1]*100))\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "CpuJBbNp32Bg"
      },
      "outputs": [],
      "source": [
        "plot_model(augmented_model_details)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "idBH1oxV33tI"
      },
      "outputs": [],
      "source": [
        "correct, labels_pred = predict_classes(augmented_model, X_test, Y_test_en)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "wuZxAMGS4AXW"
      },
      "outputs": [],
      "source": [
        "num_images = len(correct)\n",
        "print(\"Accuracy: %.2f%%\" % ((sum(correct)*100)/num_images))\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Vaw5Ikyg4Kl4"
      },
      "outputs": [],
      "source": [
        "visualize_errors(images_test, labels_test, class_names, labels_pred, correct)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "m3gdTfPyZzSg"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "provenance": []
    },
    "gpuClass": "standard",
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "name": "python",
      "version": "3.9.7 (tags/v3.9.7:1016ef3, Aug 30 2021, 20:19:38) [MSC v.1929 64 bit (AMD64)]"
    },
    "vscode": {
      "interpreter": {
        "hash": "e42b2ccb67b99ad21265aed3e7d615f64c66f1c316dfe5703bb24cc17e355247"
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
